"""噫 Pipeline Service
Serviﾃｧo principal de orquestraﾃｧﾃ｣o do pipeline de automaﾃｧﾃ｣o completa
"""

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime
from typing import Dict, Any, List, Optional

# Adicionar diretﾃｳrio routes ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'routes'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Importar serviﾃｧos necessﾃ｡rios
from services.video_creation_service import VideoCreationService
from services.checkpoint_service import CheckpointService
from services.script_processing_service import ScriptProcessingService

logger = logging.getLogger(__name__)
logger.propagate = True
logger.setLevel(logging.INFO)

def load_custom_prompts() -> Dict[str, Any]:
    """Carregar prompts personalizados do arquivo de configuraﾃｧﾃ｣o"""
    try:
        prompts_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'prompts_config.json')
        
        if os.path.exists(prompts_file):
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # Retornar prompts padrﾃ｣o se arquivo nﾃ｣o existir
            return get_default_prompts()
    except Exception as e:
        logger.warning(f"Erro ao carregar prompts personalizados: {str(e)}. Usando prompts padrﾃ｣o.")
        return get_default_prompts()

def get_default_prompts() -> Dict[str, Any]:
    """Retornar prompts padrﾃ｣o do sistema"""
    return {
        "titles": {
            "viral": "Crie tﾃｭtulos virais e chamativos que gerem curiosidade e cliques. Use tﾃｩcnicas de copywriting, nﾃｺmeros, palavras de impacto e gatilhos emocionais.",
            "educational": "Crie tﾃｭtulos educativos e informativos que transmitam conhecimento de forma clara e atrativa.",
            "entertainment": "Crie tﾃｭtulos divertidos e envolventes para entretenimento, usando humor e criatividade."
        },
        "premises": {
            "default": "Baseado no tﾃｭtulo '{title}', crie uma premissa detalhada para um vﾃｭdeo.\n\nA premissa deve:\n- Ter aproximadamente {word_count} palavras\n- Explicar o conceito principal do vﾃｭdeo\n- Definir o pﾃｺblico-alvo\n- Estabelecer o tom e estilo\n- Incluir pontos-chave a serem abordados\n- Ser envolvente e clara\n\nRetorne apenas a premissa, sem formataﾃｧﾃ｣o extra."
        },
        "scripts": {
            "default": "Crie um roteiro envolvente com {chapters} capﾃｭtulos, baseado no tﾃｭtulo '{title}' e premissa: {premise}. Duraﾃｧﾃ｣o alvo: {duration_target}."
        },
        "images": {
            "default": "Crie uma descriﾃｧﾃ｣o detalhada para geraﾃｧﾃ｣o de imagem baseada no contexto: {context}. A imagem deve ser visualmente atrativa e relevante ao conteﾃｺdo."
        }
    }

class PipelineService:
    """Serviﾃｧo principal de orquestraﾃｧﾃ｣o do pipeline"""
    
    def __init__(self, pipeline_id: str):
        self.pipeline_id = pipeline_id
        self.pipeline_state = None
        self.config = None
        self.api_keys = None
        self.results = {}
        self.custom_prompts = None
        
        # Importar estado do pipeline
        self._load_pipeline_state()
        
        # Carregar prompts personalizados
        self._load_custom_prompts()
        
        # Inicializar serviﾃｧo de checkpoint
        self.checkpoint_service = CheckpointService(pipeline_id)
        self.auto_checkpoint = self.config.get('auto_checkpoint', True) if self.config else True
        
        # Adicionar controle de threading para pausar/retomar
        import threading
        self._pause_event = threading.Event()
        self._pause_event.set()  # Iniciar como nﾃ｣o pausado
    
    def _load_custom_prompts(self):
        """Carregar prompts personalizados"""
        try:
            self.custom_prompts = load_custom_prompts()
            self._log('info', 'Prompts personalizados carregados com sucesso')
        except Exception as e:
            self._log('warning', f'Erro ao carregar prompts personalizados: {str(e)}')
            self.custom_prompts = get_default_prompts()
    
    def _load_pipeline_state(self):
        """Carregar estado do pipeline"""
        try:
            from routes.pipeline_complete import active_pipelines
            if self.pipeline_id in active_pipelines:
                self.pipeline_state = active_pipelines[self.pipeline_id]
                self.config = self.pipeline_state.get('config', {})
                self.api_keys = self.pipeline_state.get('api_keys', {})
                
                # Log de debug para verificar as etapas carregadas
                steps = self.pipeline_state.get('steps', {})
                step_names = list(steps.keys())
                self._log('info', f'Pipeline state carregado com etapas: {step_names}')
                
                # Verificar se script_processing estﾃ｡ presente
                if 'SCRIPT_PROCESSING' in steps:
                    self._log('info', 'Etapa SCRIPT_PROCESSING encontrada no estado')
                else:
                    self._log('warning', 'Etapa SCRIPT_PROCESSING Nﾃグ encontrada no estado')
            else:
                raise Exception(f"Pipeline {self.pipeline_id} nﾃ｣o encontrado")
        except Exception as e:
            logger.error(f"Erro ao carregar estado do pipeline: {str(e)}")
            raise
    
    def _log(self, level: str, message: str, data: Optional[Dict] = None):
        """Adicionar log ao pipeline"""
        try:
            from routes.pipeline_complete import add_pipeline_log
            add_pipeline_log(self.pipeline_id, level, message, data)
        except Exception as e:
            logger.error(f"Erro ao adicionar log: {str(e)}")
    
    def _save_checkpoint(self, current_step: str):
        """Salvar checkpoint do estado atual"""
        try:
            success = self.checkpoint_service.save_checkpoint(
                step=current_step,
                results=self.results,
                config=self.config,
                progress=getattr(self, 'progress', {})
            )
            
            if success:
                self._log('info', f'Checkpoint salvo para etapa: {current_step}')
            else:
                self._log('warning', f'Falha ao salvar checkpoint para etapa: {current_step}')
                
        except Exception as e:
            self._log('error', f'Erro ao salvar checkpoint: {str(e)}')
    
    def load_from_checkpoint(self) -> bool:
        """Carregar estado a partir de checkpoint"""
        try:
            checkpoint_data = self.checkpoint_service.load_checkpoint()
            
            if not checkpoint_data:
                return False
            
            # Validar integridade do checkpoint
            if not self.checkpoint_service.validate_checkpoint_integrity(checkpoint_data):
                self._log('error', 'Checkpoint invﾃ｡lido ou corrompido')
                return False
            
            # Restaurar estado
            self.results = checkpoint_data.get('results', {})
            if not hasattr(self, 'progress'):
                self.progress = {}
            self.progress.update(checkpoint_data.get('progress', {}))
            self.config.update(checkpoint_data.get('config', {}))
            
            # Criar relatﾃｳrio de recuperaﾃｧﾃ｣o
            recovery_report = self.checkpoint_service.create_recovery_report(checkpoint_data)
            
            self._log('info', 'Estado restaurado a partir de checkpoint', {
                'completed_steps': recovery_report['completed_steps'],
                'next_step': recovery_report['next_step'],
                'checkpoint_timestamp': recovery_report['checkpoint_timestamp']
            })
            
            return True
            
        except Exception as e:
            self._log('error', f'Erro ao carregar checkpoint: {str(e)}')
            return False
    
    def get_resume_info(self) -> Optional[Dict[str, Any]]:
        """Obter informaﾃｧﾃｵes para retomada da pipeline"""
        try:
            if not self.checkpoint_service.has_checkpoint():
                return None
            
            checkpoint_data = self.checkpoint_service.load_checkpoint()
            if not checkpoint_data:
                return None
            
            return self.checkpoint_service.create_recovery_report(checkpoint_data)
            
        except Exception as e:
            self._log('error', f'Erro ao obter informaﾃｧﾃｵes de retomada: {str(e)}')
            return None
    
    def _update_progress(self, step: str, progress: int, status: str = 'processing'):
        """Atualizar progresso do pipeline"""
        try:
            from routes.pipeline_complete import update_pipeline_progress
            update_pipeline_progress(self.pipeline_id, step, progress, status)
            
            # Salvar checkpoint se habilitado
            if self.auto_checkpoint and progress == 100:
                self._save_checkpoint(step)
                
        except Exception as e:
            logger.error(f"Erro ao atualizar progresso: {str(e)}")
    
    # ================================
    # 識 ETAPA 1: EXTRAﾃﾃグ DE Tﾃ控ULOS
    # ================================
    
    def run_extraction(self) -> Dict[str, Any]:
        """Executar extraﾃｧﾃ｣o de tﾃｭtulos do YouTube"""
        try:
            self._log('info', 'Iniciando extraﾃｧﾃ｣o de tﾃｭtulos do YouTube')
            
            channel_url = self.pipeline_state['channel_url']
            extraction_config = self.config.get('extraction', {})
            
            method = extraction_config.get('method', 'auto')
            max_titles_final = extraction_config.get('max_titles', 10)  # Renomeado para evitar confusﾃ｣o
            min_views = extraction_config.get('min_views', 1000)
            
            # Verificar se hﾃ｡ tﾃｭtulos prﾃｩ-fornecidos (mﾃｩtodo manual)
            if method == 'manual' and 'provided_titles' in extraction_config:
                self._log('info', 'Usando tﾃｭtulos prﾃｩ-fornecidos (mﾃｩtodo manual)')
                provided_titles = extraction_config['provided_titles']
                
                # Converter tﾃｭtulos fornecidos para o formato esperado
                titles = []
                for i, title in enumerate(provided_titles):  # Sem limite inicial
                    if isinstance(title, str):
                        titles.append({
                            'title': title,
                            'video_id': f'manual_{i}',
                            'views': min_views + 1000,  # Garantir que passa no filtro
                            'description': '',
                            'thumbnail': '',
                            'duration': '5:00',
                            'likes': 100,
                            'published_at': datetime.utcnow().isoformat(),
                            'url': f'https://youtube.com/watch?v=manual_{i}'
                        })
                    elif isinstance(title, dict):
                        titles.append(title)
                
                # Filtrar tﾃｭtulos com visualizaﾃｧﾃｵes insuficientes (apesar de termos definido como min_views + 1000)
                if min_views > 0:
                    titles = [t for t in titles if t.get('views', 0) >= min_views]
                
                # Aplicar limite final
                titles = titles[:max_titles_final]
                
                self._update_progress('extraction', 100)
                
                extraction_result = {
                    'channel_info': {'name': 'Manual Input', 'id': 'manual'},
                    'titles': titles,
                    'total_extracted': len(titles),
                    'method_used': 'manual',
                    'extraction_time': datetime.utcnow().isoformat()
                }
                
                self.results['extraction'] = extraction_result
                
                self._log('info', f'Extraﾃｧﾃ｣o manual concluﾃｭda: {len(titles)} tﾃｭtulos fornecidos', {
                    'total_titles': len(titles),
                    'method': 'manual'
                })
                
                return extraction_result
            
            # Importar funﾃｧﾃ｣o de extraﾃｧﾃ｣o para mﾃｩtodos automﾃ｡ticos
            from routes.automations import get_channel_videos_ytdlp, get_channel_videos_rapidapi, get_next_rapidapi_key, extract_channel_id_from_url
            
            self._update_progress('extraction', 25)
            
            # Tentar extraﾃｧﾃ｣o baseada no mﾃｩtodo configurado
            # IMPORTANTE: Extrair com uma quantidade maior para ter amostra suficiente para filtragem
            extraction_limit = max(max_titles_final * 5, 50)  # Extrair pelo menos 50 ou 5x o valor final desejado
            
            if method in ['ytdlp', 'yt-dlp'] or method == 'auto':
                self._log('info', f'Tentando extraﾃｧﾃ｣o com yt-dlp (extraindo {extraction_limit} vﾃｭdeos para filtragem)')
                result = get_channel_videos_ytdlp(channel_url, extraction_limit)
                
                if not result.get('success') and method == 'auto':
                    self._log('warning', 'yt-dlp falhou, tentando RapidAPI como fallback')
                    self._update_progress('extraction', 50)
                    # Obter chave RapidAPI e extrair channel_id
                    api_key = get_next_rapidapi_key()
                    if not api_key:
                        raise Exception("Nenhuma chave RapidAPI disponﾃｭvel")
                    channel_id = extract_channel_id_from_url(channel_url, api_key)
                    if not channel_id:
                        raise Exception(f"Nﾃ｣o foi possﾃｭvel extrair channel_id da URL: {channel_url}")
                    result = get_channel_videos_rapidapi(channel_id, api_key, extraction_limit)
            
            elif method == 'rapidapi':
                self._log('info', f'Usando extraﾃｧﾃ｣o com RapidAPI (extraindo {extraction_limit} vﾃｭdeos para filtragem)')
                # Obter chave RapidAPI e extrair channel_id
                api_key = get_next_rapidapi_key()
                if not api_key:
                    raise Exception("Nenhuma chave RapidAPI disponﾃｭvel")
                channel_id = extract_channel_id_from_url(channel_url, api_key)
                if not channel_id:
                    raise Exception(f"Nﾃ｣o foi possﾃｭvel extrair channel_id da URL: {channel_url}")
                result = get_channel_videos_rapidapi(channel_id, api_key, extraction_limit)
            
            else:
                raise Exception(f"Mﾃｩtodo de extraﾃｧﾃ｣o invﾃ｡lido: {method}")
            
            if not result.get('success'):
                raise Exception(f"Falha na extraﾃｧﾃ｣o: {result.get('error', 'Erro desconhecido')}")
            
            # Filtrar por visualizaﾃｧﾃｵes mﾃｭnimas (ETAPA 1: FILTRAGEM)
            titles_data = result.get('data', {})
            # Verificar se os dados estﾃ｣o em 'titles' ou 'videos'
            titles = titles_data.get('titles', titles_data.get('videos', []))
            
            original_count = len(titles)
            if min_views > 0:
                # Filtrar tﾃｭtulos com visualizaﾃｧﾃｵes mﾃｭnimas
                filtered_titles = [title for title in titles if title.get('views', 0) >= min_views]
                self._log('info', f'Filtrados {len(filtered_titles)} tﾃｭtulos com mais de {min_views} visualizaﾃｧﾃｵes (de {original_count} extraﾃｭdos)')
                
                # Aplicar limite final apﾃｳs filtragem
                titles = filtered_titles[:max_titles_final]
            else:
                # Aplicar limite final se nﾃ｣o houver filtro por visualizaﾃｧﾃｵes
                titles = titles[:max_titles_final]
            
            # Verificar se algum tﾃｭtulo passou no filtro
            if not titles:
                # Criar tﾃｭtulos simulados se nenhum tﾃｭtulo passou no filtro
                self._log('warning', 'Nenhum tﾃｭtulo passou no filtro, criando tﾃｭtulos simulados')
                # Criar tﾃｭtulos simulados
                titles = []
                for i in range(max_titles_final):
                    titles.append({
                        'title': f'Tﾃｭtulo simulado {i+1}',
                        'video_id': f'simulado_{i}',
                        'views': min_views + 1000,  # Garantir que passa no filtro
                        'description': 'Descriﾃｧﾃ｣o simulada para teste',
                        'thumbnail': '',
                        'duration': '5:00',
                        'likes': 100,
                        'published_at': datetime.utcnow().isoformat(),
                        'url': f'https://youtube.com/watch?v=simulado_{i}'
                    })
            
            self._update_progress('extraction', 100)
            
            extraction_result = {
                'channel_info': titles_data.get('channel_info', {}),
                'titles': titles,
                'total_extracted': len(titles),
                'method_used': result.get('method', method),
                'extraction_time': datetime.utcnow().isoformat()
            }
            
            self.results['extraction'] = extraction_result
            
            self._log('info', f'Extraﾃｧﾃ｣o concluﾃｭda: {len(titles)} tﾃｭtulos extraﾃｭdos apﾃｳs filtragem e limitaﾃｧﾃ｣o', {
                'total_titles': len(titles),
                'method': result.get('method', method),
                'channel': titles_data.get('channel_info', {}).get('name', 'Desconhecido'),
                'min_views': min_views
            })
            
            return extraction_result
            
        except Exception as e:
            self._log('error', f'Erro na extraﾃｧﾃ｣o: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 2: GERAﾃﾃグ DE Tﾃ控ULOS
    # ================================
    
    def run_titles_generation(self) -> Dict[str, Any]:
        """Executar geraﾃｧﾃ｣o de novos tﾃｭtulos"""
        try:
            self._log('info', 'Iniciando geraﾃｧﾃ｣o de novos tﾃｭtulos')
            
            # Verificar se temos tﾃｭtulos extraﾃｭdos
            if 'extraction' not in self.results:
                raise Exception('Extraﾃｧﾃ｣o de tﾃｭtulos nﾃ｣o foi executada')
            
            extracted_titles = self.results['extraction']['titles']
            if not extracted_titles:
                raise Exception('Nenhum tﾃｭtulo foi extraﾃｭdo')
            
            titles_config = self.config.get('titles', {})
            provider = titles_config.get('provider', 'gemini')
            count = titles_config.get('count', 5)
            style = titles_config.get('style', 'viral')
            custom_prompt = titles_config.get('custom_prompt', False)
            
            self._update_progress('titles', 25)
            
            # Preparar tﾃｭtulos de origem
            source_titles = [title.get('title', '') for title in extracted_titles]
            
            # PRIORIDADE DE PROMPTS: Custom > Agent > System Default
            prompt_source = 'system_default'
            instructions = ''
            
            # 1. PRIMEIRA PRIORIDADE: Custom prompt do usuﾃ｡rio
            if custom_prompt and 'custom_instructions' in titles_config:
                instructions = titles_config['custom_instructions']
                prompt_source = 'custom_user'
                self._log('info', f'鹿 Usando prompt personalizado do usuﾃ｡rio para tﾃｭtulos')
            
            # 2. SEGUNDA PRIORIDADE: Agent prompt especializado
            elif 'agent_prompts' in titles_config and style in titles_config['agent_prompts']:
                instructions = titles_config['agent_prompts'][style]
                prompt_source = 'agent_specialized'
                agent_info = self.config.get('agent', {})
                agent_name = agent_info.get('name', 'Agente Especializado')
                self._log('info', f'紙 Usando prompt do agente "{agent_name}" - Estilo: {style}')
            
            # 3. TERCEIRA PRIORIDADE: System default
            else:
                # Usar prompts personalizados carregados ou padrﾃ｣o do sistema
                titles_prompts = self.custom_prompts.get('titles', {})
                instructions = titles_prompts.get(style, titles_prompts.get('viral', 'Crie tﾃｭtulos virais e chamativos que gerem curiosidade e cliques'))
                prompt_source = 'system_default'
                self._log('info', f'統 Usando prompt padrﾃ｣o do sistema - Estilo: {style}')
            
            self._update_progress('titles', 50)
            
            # Log do provedor sendo usado
            self._log('info', f'Provedor de IA para tﾃｭtulos: {provider}')
            
            # Gerar tﾃｭtulos usando o provedor configurado
            if provider == 'gemini':
                from services.ai_services import generate_titles_with_gemini
                # Forﾃｧar reset das chaves Gemini para garantir disponibilidade
                try:
                    from routes.automations import GEMINI_KEYS_ROTATION
                    from datetime import datetime
                    GEMINI_KEYS_ROTATION['usage_count'] = {}
                    GEMINI_KEYS_ROTATION['current_index'] = 0
                    GEMINI_KEYS_ROTATION['last_reset'] = datetime.now().date()
                    self._log('info', 'Reset das chaves Gemini aplicado na pipeline')
                except Exception as reset_error:
                    self._log('warning', f'Erro no reset das chaves Gemini: {reset_error}')
                
                # Usar sistema de rotaﾃｧﾃ｣o de chaves em vez de chave fixa
                api_key = None  # Deixar None para usar get_next_gemini_key() automaticamente
                def update_titles_partial(current_titles):
                    self.results['titles'] = {'generated_titles': current_titles, 'partial': True}
                    progress = int((len(current_titles) / count) * 100)
                    self._update_progress('titles', progress)
                result = generate_titles_with_gemini(source_titles, instructions, api_key, update_callback=update_titles_partial, count=count)
            
            elif provider == 'auto':
                # Tentar OpenAI primeiro, depois Gemini em caso de erro
                try:
                    from services.ai_services import generate_titles_with_openai
                    api_key = self.api_keys.get('openai')
                    def update_titles_partial(current_titles):
                        self.results['titles'] = {'generated_titles': current_titles, 'partial': True}
                        progress = int((len(current_titles) / count) * 100)
                        self._update_progress('titles', progress)
                    result = generate_titles_with_openai(source_titles, instructions, api_key, update_callback=update_titles_partial)
                    self._log('info', 'Tﾃｭtulos gerados com OpenAI (auto mode)')
                except Exception as e:
                    error_msg = str(e).lower()
                    if '429' in error_msg or 'quota' in error_msg or 'insufficient_quota' in error_msg:
                        self._log('warning', f'OpenAI falhou (quota excedida), tentando Gemini: {str(e)}')
                        try:
                            from services.ai_services import generate_titles_with_gemini
                            # Forﾃｧar reset das chaves Gemini para garantir disponibilidade
                            try:
                                from routes.automations import GEMINI_KEYS_ROTATION
                                from datetime import datetime
                                GEMINI_KEYS_ROTATION['usage_count'] = {}
                                GEMINI_KEYS_ROTATION['current_index'] = 0
                                GEMINI_KEYS_ROTATION['last_reset'] = datetime.now().date()
                                self._log('info', 'Reset das chaves Gemini aplicado na pipeline (fallback)')
                            except Exception as reset_error:
                                self._log('warning', f'Erro no reset das chaves Gemini: {reset_error}')
                            
                            # Usar sistema de rotaﾃｧﾃ｣o de chaves em vez de chave fixa
                            api_key = None  # Deixar None para usar get_next_gemini_key() automaticamente
                            def update_titles_partial(current_titles):
                                self.results['titles'] = {'generated_titles': current_titles, 'partial': True}
                                progress = int((len(current_titles) / count) * 100)
                                self._update_progress('titles', progress)
                            result = generate_titles_with_gemini(source_titles, instructions, api_key, update_callback=update_titles_partial, count=count)
                            self._log('info', 'Tﾃｭtulos gerados com Gemini (fallback)')
                        except Exception as gemini_error:
                            self._log('error', f'Gemini tambﾃｩm falhou: {str(gemini_error)}')
                            raise Exception(f'Ambos provedores falharam - OpenAI: {str(e)}, Gemini: {str(gemini_error)}')
                    else:
                        raise e
            
            else:
                raise Exception(f"Provedor de IA invﾃ｡lido: {provider}")
            
            if not result.get('success'):
                raise Exception(f"Falha na geraﾃｧﾃ｣o de tﾃｭtulos: {result.get('error', 'Erro desconhecido')}")
            
            self._update_progress('titles', 100)
            
            titles_result = {
                'generated_titles': result['data']['generated_titles'][:count],
                'source_titles_count': len(source_titles),
                'provider_used': provider,
                'style': style,
                'prompt_source': prompt_source,  # Indicar origem do prompt
                'agent_info': self.config.get('agent', {}) if prompt_source == 'agent_specialized' else None,
                'generation_time': datetime.utcnow().isoformat(),
                'selected_title': result['data']['generated_titles'][0] if result['data']['generated_titles'] else None,
                'original_title': extracted_titles[0].get('title', '') if extracted_titles else None,
                'original_video_url': extracted_titles[0].get('url', '') if extracted_titles else None
            }
            
            self.results['titles'] = titles_result
            
            self._log('info', f'Geraﾃｧﾃ｣o de tﾃｭtulos concluﾃｭda: {len(titles_result["generated_titles"])} tﾃｭtulos gerados', {
                'provider': provider,
                'style': style,
                'count': len(titles_result['generated_titles'])
            })
            
            return titles_result
            
        except Exception as e:
            self._log('error', f'Erro na geraﾃｧﾃ｣o de tﾃｭtulos: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 3: GERAﾃﾃグ DE PREMISSAS
    # ================================
    
    def run_premises_generation(self) -> Dict[str, Any]:
        """Executar geraﾃｧﾃ｣o de premissas"""
        try:
            self._log('info', 'Iniciando geraﾃｧﾃ｣o de premissas')
            
            # Verificar se temos tﾃｭtulos gerados
            if 'titles' not in self.results:
                raise Exception('Geraﾃｧﾃ｣o de tﾃｭtulos nﾃ｣o foi executada')
            
            generated_titles = self.results['titles']['generated_titles']
            if not generated_titles:
                raise Exception('Nenhum tﾃｭtulo foi gerado')
            
            premises_config = self.config.get('premises', {})
            provider = premises_config.get('provider', 'gemini')
            word_count = premises_config.get('word_count', 200)
            custom_prompt = premises_config.get('custom_prompt', False)
            
            self._update_progress('premises', 25)
            
            # Selecionar o melhor tﾃｭtulo (primeiro da lista)
            selected_title = generated_titles[0]
            
            # PRIORIDADE DE PROMPTS: Custom > Agent > System Default
            prompt_source = 'system_default'
            instructions = ''
            
            # 1. PRIMEIRA PRIORIDADE: Custom prompt do usuﾃ｡rio
            if custom_prompt and 'custom_instructions' in premises_config:
                instructions = premises_config['custom_instructions']
                prompt_source = 'custom_user'
                self._log('info', f'鹿 Usando prompt personalizado do usuﾃ｡rio para premissas')
            
            # 2. SEGUNDA PRIORIDADE: Agent prompt especializado
            elif 'agent_prompts' in premises_config:
                # Para premissas, verificar estilos disponﾃｭveis no agente
                agent_prompts = premises_config['agent_prompts']
                
                # Tentar encontrar um estilo compatﾃｭvel (narrative, educational)
                # Now we properly use the selected style from the form
                premise_style = premises_config.get('style', 'educational')
                if premise_style in agent_prompts:
                    selected_style = premise_style
                elif 'educational' in agent_prompts:
                    selected_style = 'educational'
                elif 'narrative' in agent_prompts:
                    selected_style = 'narrative'
                else:
                    # Se agente nﾃ｣o tem estilos compatﾃｭveis, usar primeiro disponﾃｭvel
                    selected_style = list(agent_prompts.keys())[0] if agent_prompts else None
                
                if selected_style and selected_style in agent_prompts:
                    # Formatar prompt do agente com variﾃ｡veis
                    agent_prompt_template = agent_prompts[selected_style]
                    instructions = agent_prompt_template.format(
                        title=selected_title,
                        word_count=word_count
                    )
                    prompt_source = 'agent_specialized'
                    agent_info = self.config.get('agent', {})
                    agent_name = agent_info.get('name', 'Agente Especializado')
                    self._log('info', f'紙 Usando prompt do agente "{agent_name}" - Estilo: {selected_style}')
                else:
                    # Fallback para sistema se agente nﾃ｣o tem estilos compatﾃｭveis
                    premises_prompts = self.custom_prompts.get('premises', {})
                    prompt_template = premises_prompts.get('default', 
                        "Baseado no tﾃｭtulo '{title}', crie uma premissa detalhada para um vﾃｭdeo.\n\nA premissa deve:\n- Ter aproximadamente {word_count} palavras\n- Explicar o conceito principal do vﾃｭdeo\n- Definir o pﾃｺblico-alvo\n- Estabelecer o tom e estilo\n- Incluir pontos-chave a serem abordados\n- Ser envolvente e clara\n\nRetorne apenas a premissa, sem formataﾃｧﾃ｣o extra.")
                    
                    instructions = prompt_template.format(
                        title=selected_title,
                        word_count=word_count
                    )
                    prompt_source = 'system_default'
                    self._log('info', f'統 Usando prompt padrﾃ｣o do sistema (agente sem estilos compatﾃｭveis)')
            
            # 3. TERCEIRA PRIORIDADE: System default
            else:
                # Usar prompt personalizado carregado
                premises_prompts = self.custom_prompts.get('premises', {})
                prompt_template = premises_prompts.get('default', 
                    "Baseado no tﾃｭtulo '{title}', crie uma premissa detalhada para um vﾃｭdeo.\n\nA premissa deve:\n- Ter aproximadamente {word_count} palavras\n- Explicar o conceito principal do vﾃｭdeo\n- Definir o pﾃｺblico-alvo\n- Estabelecer o tom e estilo\n- Incluir pontos-chave a serem abordados\n- Ser envolvente e clara\n\nRetorne apenas a premissa, sem formataﾃｧﾃ｣o extra.")
                
                # Substituir variﾃ｡veis no template
                instructions = prompt_template.format(
                    title=selected_title,
                    word_count=word_count
                )
                prompt_source = 'system_default'
                self._log('info', f'統 Usando prompt padrﾃ｣o do sistema para premissas')
            
            self._update_progress('premises', 50)
            
            # Gerar premissa usando o provedor configurado
            if provider == 'gemini':
                import google.generativeai as genai
                # Usar sistema de rotaﾃｧﾃ｣o de chaves diretamente
                from routes.automations import get_next_gemini_key, handle_gemini_429_error, get_fallback_provider_info
                api_key = get_next_gemini_key()
                
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel('gemini-1.5-flash')
                premise_text = ''
                for chunk in model.generate_content(instructions, stream=True):
                    premise_text += chunk.text
                    self.results['premises'] = {'premise': premise_text, 'partial': True}
                    # Atualizar progresso baseado em comprimento aproximado
                    current_length = len(premise_text.split())
                    progress = min(int((current_length / word_count) * 100), 99)
                    self._update_progress('premises', progress)
            
            elif provider == 'openai':
                import openai
                api_key = self.api_keys.get('openai')
                client = openai.OpenAI(api_key=api_key)
                
                premise_text = ''
                stream = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": instructions}],
                    max_tokens=500,
                    temperature=0.7,
                    stream=True
                )
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        premise_text += chunk.choices[0].delta.content
                        self.results['premises'] = {'premise': premise_text, 'partial': True}
                        current_length = len(premise_text.split())
                        progress = min(int((current_length / word_count) * 100), 99)
                        self._update_progress('premises', progress)
            
            elif provider == 'auto':
                # Tentar OpenAI primeiro, depois Gemini em caso de erro
                try:
                    import openai
                    api_key = self.api_keys.get('openai')
                    client = openai.OpenAI(api_key=api_key)
                    
                    premise_text = ''
                    stream = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": instructions}],
                        max_tokens=500,
                        temperature=0.7,
                        stream=True
                    )
                    for chunk in stream:
                        if chunk.choices[0].delta.content is not None:
                            premise_text += chunk.choices[0].delta.content
                            self.results['premises'] = {'premise': premise_text, 'partial': True}
                            current_length = len(premise_text.split())
                            progress = min(int((current_length / word_count) * 100), 99)
                            self._update_progress('premises', progress)
                    self._log('info', 'Premissa gerada com OpenAI (auto mode)')
                except Exception as e:
                    error_msg = str(e).lower()
                    if '429' in error_msg or 'quota' in error_msg or 'insufficient_quota' in error_msg:
                        self._log('warning', f'OpenAI falhou (quota excedida), tentando Gemini: {str(e)}')
                        # Tentar Gemini como fallback primﾃ｡rio
                        try:
                            import google.generativeai as genai
                            api_key = self.api_keys.get('gemini')
                            if not api_key:
                                api_key = get_next_gemini_key()
                            
                            if not api_key:
                                raise Exception('Nenhuma chave Gemini disponﾃｭvel para fallback.')

                            genai.configure(api_key=api_key)
                            model = genai.GenerativeModel('gemini-1.5-flash')
                            premise_text = ''
                            for chunk in model.generate_content(instructions, stream=True):
                                premise_text += chunk.text
                                self.results['premises'] = {'premise': premise_text, 'partial': True}
                                current_length = len(premise_text.split())
                                progress = min(int((current_length / word_count) * 100), 99)
                                self._update_progress('premises', progress)
                            self._log('info', 'Premissa gerada com Gemini (fallback)')
                        except Exception as gemini_error:
                            self._log('error', f'Gemini tambﾃｩm falhou: {str(gemini_error)}')
                            # Se Gemini falhar, tentar OpenRouter ou OpenAI
                            fallback_info = get_fallback_provider_info()
                            if fallback_info:
                                fallback_provider = fallback_info['provider']
                                fallback_key = fallback_info['key']
                                self._log('warning', f'Tentando fallback para {fallback_provider}...')
                                try:
                                    if fallback_provider == 'openrouter':
                                        client = openai.OpenAI(
                                            base_url="https://openrouter.ai/api/v1",
                                            api_key=fallback_key,
                                        )
                                        model_name = "mistralai/mistral-7b-instruct"
                                    elif fallback_provider == 'openai':
                                        client = openai.OpenAI(api_key=fallback_key)
                                        model_name = "gpt-3.5-turbo"
                                    
                                    premise_text = ''
                                    stream = client.chat.completions.create(
                                        model=model_name,
                                        messages=[{"role": "user", "content": instructions}],
                                        max_tokens=500,
                                        temperature=0.7,
                                        stream=True
                                    )
                                    for chunk in stream:
                                        if chunk.choices[0].delta.content is not None:
                                            premise_text += chunk.choices[0].delta.content
                                            self.results['premises'] = {'premise': premise_text, 'partial': True}
                                            current_length = len(premise_text.split())
                                            progress = min(int((current_length / word_count) * 100), 99)
                                            self._update_progress('premises', progress)
                                    self._log('info', f'Premissa gerada com {fallback_provider} (fallback)')
                                except Exception as fallback_e:
                                    self._log('error', f'Fallback para {fallback_provider} tambﾃｩm falhou: {str(fallback_e)}')
                                    raise Exception(f'Todos os provedores falharam - OpenAI: {str(e)}, Gemini: {str(gemini_error)}, Fallback ({fallback_provider}): {str(fallback_e)}')
                            else:
                                raise Exception(f'Ambos provedores falharam e nenhum fallback disponﾃｭvel - OpenAI: {str(e)}, Gemini: {str(gemini_error)}')
                        except Exception as inner_e:
                            self._log('error', f'Erro no fallback Gemini: {str(inner_e)}')
                            raise e
                    else:
                        raise e
            
            else:
                raise Exception(f"Provedor de IA invﾃ｡lido: {provider}")
            
            self._update_progress('premises', 100)
            
            premises_result = {
                'selected_title': selected_title,
                'premises': [{
                    'title': selected_title,
                    'premise': premise_text,
                    'word_count': len(premise_text.split())
                }],
                'premise': premise_text,  # Manter compatibilidade
                'word_count': len(premise_text.split()),
                'provider_used': provider,
                'style': premises_config.get('style', 'educational'),  # Add style information
                'prompt_source': prompt_source,  # Indicar origem do prompt
                'agent_info': self.config.get('agent', {}) if prompt_source == 'agent_specialized' else None,
                'generation_time': datetime.utcnow().isoformat()
            }
            
            self.results['premises'] = premises_result
            
            self._log('info', 'Geraﾃｧﾃ｣o de premissas concluﾃｭda', {
                'title': selected_title,
                'word_count': len(premise_text.split()),
                'provider': provider
            })
            
            return premises_result
            
        except Exception as e:
            self._log('error', f'Erro na geraﾃｧﾃ｣o de premissas: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 4: GERAﾃﾃグ DE ROTEIROS
    # ================================
    
    def run_scripts_generation(self) -> Dict[str, Any]:
        """Executar geraﾃｧﾃ｣o de roteiros"""
        try:
            self._log('info', 'Iniciando geraﾃｧﾃ｣o de roteiros')
            
            # Verificar se temos premissa
            if 'premises' not in self.results:
                raise Exception('Geraﾃｧﾃ｣o de premissas nﾃ｣o foi executada')
            
            premise_data = self.results['premises']
            title = premise_data['selected_title']
            premise = premise_data['premise']
            
            scripts_config = self.config.get('scripts', {})
            chapters = scripts_config.get('chapters', 5)
            style = scripts_config.get('style', 'inicio')
            duration_target = scripts_config.get('duration_target', '5-7 minutes')
            include_hooks = scripts_config.get('include_hooks', True)
            
            self._update_progress('scripts', 25)
            
            # Importar gerador de roteiros
            from routes.scripts import generate_long_script
            
            # Importar funﾃｧﾃｵes auxiliares para prompts de roteiro
            from routes.premise import create_inicio_prompt, create_capitulo_prompt, create_final_prompt
            
            # Obter prompts personalizados de roteiros
            scripts_config = self.config.get('scripts', {})
            custom_inicio = scripts_config.get('custom_inicio', '')
            custom_meio = scripts_config.get('custom_meio', '')
            custom_fim = scripts_config.get('custom_fim', '')
            
            # Preparar dados para geraﾃｧﾃ｣o
            script_data = {
                'title': title,
                'premise': premise,
                'chapters': chapters,
                'style': style,
                'duration_target': duration_target,
                'include_hooks': include_hooks,
                'custom_inicio': custom_inicio,
                'custom_meio': custom_meio,
                'custom_fim': custom_fim,
                'detailed_prompt_text': scripts_config.get('detailed_prompt_text', ''),
                'detailed_prompt': scripts_config.get('detailed_prompt', False),
                'contextual_chapters': scripts_config.get('contextual_chapters', False)
            }
            
            self._update_progress('scripts', 50)
            
            def update_scripts_partial(chapters):
                partial_data = {
                    'chapters': chapters,
                    'partial': True,
                    'chapters_generated': len(chapters)
                }
                self.results['scripts'] = partial_data
                if 'results' not in self.pipeline_state:
                    self.pipeline_state['results'] = {}
                self.pipeline_state['results']['scripts'] = partial_data
                progress = int((len(chapters) / max(1, num_chapters)) * 100) if len(chapters) > 0 else 0
                self._update_progress('scripts', progress)

            # Usar Storyteller Unlimited para 100% dos roteiros
            from services.storyteller_service import StorytellerService
            
            # Obter configuraﾃｧﾃｵes do Storyteller com fallback para configuraﾃｧﾃｵes legadas
            agent_type = scripts_config.get('storyteller_agent') or scripts_config.get('agent', 'millionaire_stories')
            num_chapters = scripts_config.get('storyteller_chapters') or scripts_config.get('chapters', 5)
            
            # Criar instﾃ｢ncia do serviﾃｧo
            storyteller_service = StorytellerService()
            
            # Preparar premissa com instruﾃｧﾃｵes customizadas se fornecidas
            custom_instructions = scripts_config.get('custom_instructions')
            if custom_instructions:
                enhanced_premise = f"{premise}\n\nInstruﾃｧﾃｵes adicionais: {custom_instructions}"
            else:
                enhanced_premise = premise
            
            # Adicionar prompts personalizados da configuraﾃｧﾃ｣o como contexto adicional
            custom_context = ""
            if scripts_config.get('custom_prompt', False):
                custom_inicio = scripts_config.get('custom_inicio', '')
                custom_meio = scripts_config.get('custom_meio', '')
                custom_fim = scripts_config.get('custom_fim', '')
                
                if custom_inicio or custom_meio or custom_fim:
                    custom_context = f"\n\nContexto adicional:\nInﾃｭcio: {custom_inicio}\nMeio: {custom_meio}\nFim: {custom_fim}"
                    enhanced_premise += custom_context
            
            # Gerar roteiro com Storyteller Unlimited usando rotaﾃｧﾃ｣o automﾃ｡tica de chaves
            result = storyteller_service.generate_storyteller_script(
                title=title,
                premise=enhanced_premise,
                agent_type=agent_type,
                num_chapters=num_chapters,
                provider='gemini',
                progress_callback=update_scripts_partial,
                remove_chapter_headers=True  # Remove cabeﾃｧalhos de capﾃｭtulos do roteiro final
                # api_key nﾃ｣o ﾃｩ mais necessﾃ｡rio - usa rotaﾃｧﾃ｣o automﾃ｡tica
            )
            
            # Converter formato para compatibilidade
            if result.get('success'):
                result = {
                    'success': True,
                    'data': {
                        'script': result.get('full_script', ''),
                        'chapters': result.get('chapters', []),
                        'estimated_duration': result.get('estimated_duration', '5-7 minutes')
                    }
                }
            
            if not result.get('success'):
                raise Exception(f"Falha na geraﾃｧﾃ｣o de roteiro: {result.get('error', 'Erro desconhecido')}")
            
            self._update_progress('scripts', 100)
            
            scripts_result = {
                'title': title,
                'premise': premise,
                'script': result['data']['script'],
                'chapters': result['data'].get('chapters', []),
                'chapters_generated': len(result['data'].get('chapters', [])),
                'estimated_duration': result['data'].get('estimated_duration', duration_target),
                'style': style,
                'prompt_source': 'system_default',  # Default value, will be updated if using agent prompts
                'agent_info': None,  # Will be updated if using agent prompts
                'generation_time': datetime.utcnow().isoformat(),
                'partial': False
            }
            
            # Add prompt source information if using agent prompts
            scripts_config = self.config.get('scripts', {})
            if 'agent_prompts' in scripts_config:
                scripts_result['prompt_source'] = 'agent_specialized'
                scripts_result['agent_info'] = self.config.get('agent', {})
            
            self.results['scripts'] = scripts_result
            
            self._log('info', 'Geraﾃｧﾃ｣o de roteiros concluﾃｭda', {
                'chapters': scripts_result['chapters_generated'],
                'style': style,
                'estimated_duration': scripts_result['estimated_duration']
            })
            
            return scripts_result
            
        except Exception as e:
            self._log('error', f'Erro na geraﾃｧﾃ｣o de roteiros: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 5: PROCESSAMENTO DE ROTEIRO
    # ================================
    
    def run_script_processing(self) -> Dict[str, Any]:
        """Executar processamento e limpeza de roteiro"""
        try:
            self._log('info', 'Iniciando processamento de roteiro')
            
            # Verificar se processamento estﾃ｡ habilitado na configuraﾃｧﾃ｣o
            script_processing_config = self.config.get('script_processing', {})
            if not script_processing_config.get('enabled', True):
                self._log('info', 'Processamento de roteiro desabilitado na configuraﾃｧﾃ｣o, pulando etapa')
                
                # Usar roteiro original sem processamento
                if 'scripts' not in self.results:
                    raise Exception('Geraﾃｧﾃ｣o de roteiros nﾃ｣o foi executada')
                
                script_data = self.results['scripts']
                processed_result = {
                    'processed_script': script_data['script'],
                    'original_script': script_data['script'],
                    'processing_applied': False,
                    'metrics': {
                        'original_length': len(script_data['script']),
                        'processed_length': len(script_data['script']),
                        'preservation_ratio': 1.0,
                        'headers_removed': 0
                    },
                    'processing_time': 0,
                    'status': 'skipped',
                    'message': 'Processamento desabilitado pelo usuﾃ｡rio',
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                self.results['script_processing'] = processed_result
                self._update_progress('script_processing', 100)
                return processed_result
            
            # Verificar se temos roteiro
            if 'scripts' not in self.results:
                raise Exception('Geraﾃｧﾃ｣o de roteiros nﾃ｣o foi executada')
            
            script_data = self.results['scripts']
            raw_script = script_data['script']
            
            self._update_progress('script_processing', 25)
            
            # Criar instﾃ｢ncia do serviﾃｧo de processamento
            script_processor = ScriptProcessingService()
            
            self._update_progress('script_processing', 50)
            
            # Processar roteiro
            processing_result = script_processor.process_script(
                pipeline_id=self.pipeline_id,
                raw_script=raw_script,
                config=script_processing_config
            )
            
            self._update_progress('script_processing', 75)
            
            # Verificar se o processamento foi bem-sucedido
            if not processing_result.get('success'):
                raise Exception(f"Falha no processamento de roteiro: {processing_result.get('error', 'Erro desconhecido')}")
            
            # Atualizar resultado dos scripts com versﾃ｣o processada
            self.results['scripts']['script'] = processing_result['processed_script']
            
            # Salvar resultado do processamento
            script_processing_result = {
                'processed_script': processing_result['processed_script'],
                'original_script': raw_script,
                'processing_applied': True,
                'metrics': processing_result['metrics'],
                'processing_time': processing_result['processing_time'],
                'config_used': processing_result['config_used'],
                'status': 'completed',
                'timestamp': processing_result['timestamp']
            }
            
            self.results['script_processing'] = script_processing_result
            
            self._update_progress('script_processing', 100)
            
            self._log('info', 'Processamento de roteiro concluﾃｭdo', {
                'original_length': len(raw_script),
                'processed_length': len(processing_result['processed_script']),
                'preservation_ratio': processing_result['metrics']['preservation_ratio'],
                'headers_removed': processing_result['metrics']['headers_removed'],
                'processing_time': processing_result['processing_time']
            })
            
            return script_processing_result
            
        except Exception as e:
            self._log('error', f'Erro no processamento de roteiro: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 6: GERAﾃﾃグ DE TTS
    # ================================
    
    def run_tts_generation(self) -> Dict[str, Any]:
        """Executar geraﾃｧﾃ｣o de ﾃ｡udio TTS"""
        try:
            self._log('info', 'Iniciando geraﾃｧﾃ｣o de ﾃ｡udio TTS')
            
            # Verificar se TTS estﾃ｡ habilitado na configuraﾃｧﾃ｣o
            tts_config = self.config.get('tts', {})
            if not tts_config.get('enabled', True):
                self._log('info', 'TTS desabilitado na configuraﾃｧﾃ｣o, pulando etapa')
                
                # Criar resultado placeholder para compatibilidade
                tts_result = {
                    'audio_file_path': None,
                    'duration': 0,
                    'file_size': 0,
                    'provider_used': 'disabled',
                    'voice': 'none',
                    'speed': 1.0,
                    'emotion': 'neutral',
                    'generation_time': datetime.utcnow().isoformat(),
                    'status': 'skipped',
                    'message': 'TTS desabilitado pelo usuﾃ｡rio'
                }
                
                self.results['tts'] = tts_result
                self._update_progress('tts', 100)
                return tts_result
            
            # Verificar se temos roteiro
            if 'scripts' not in self.results:
                raise Exception('Geraﾃｧﾃ｣o de roteiros nﾃ｣o foi executada')
            
            script_data = self.results['scripts']
            script_text = script_data['script']
            
            tts_config = self.config.get('tts', {})
            provider = tts_config.get('provider', 'kokoro')
            voice = tts_config.get('voice', 'default')
            language = tts_config.get('language', 'en')  # Adicionar configuraﾃｧﾃ｣o de idioma
            
            # Mapear voz 'default' para vozes vﾃ｡lidas por provedor
            if voice == 'default':
                if provider == 'kokoro':
                    voice = 'af_bella'  # Voz padrﾃ｣o vﾃ｡lida para Kokoro
                elif provider == 'elevenlabs':
                    voice = 'Rachel'  # Voz padrﾃ｣o vﾃ｡lida para ElevenLabs
                elif provider == 'gemini':
                    voice = 'en-US-Journey-F'  # Voz padrﾃ｣o vﾃ｡lida para Gemini
            
            speed = tts_config.get('speed', 1.0)
            emotion = tts_config.get('emotion', 'neutral')
            
            self._update_progress('tts', 25)
            
            # Importar serviﾃｧo de TTS
            from services.tts_service import TTSService
            
            tts_service = TTSService(self.pipeline_id)
            
            self._update_progress('tts', 50)
            
            # Gerar TTS usando o serviﾃｧo
            result = tts_service.generate_tts_for_script(
                script_text=script_text,
                provider=provider,
                voice_settings={
                    'voice': voice,
                    'speed': speed,
                    'emotion': emotion,
                    'language': language  # Adicionar idioma nas configuraﾃｧﾃｵes
                }
            )
            
            self._update_progress('tts', 100)
            
            # Verificar se o resultado do TTS foi bem-sucedido
            if not result.get('success'):
                raise Exception(f"Falha na geraﾃｧﾃ｣o de TTS: {result.get('error', 'Erro desconhecido')}")
            
            # Extrair nome do arquivo para criar URL
            audio_filename = None
            if result.get('audio_file_path'):
                audio_filename = os.path.basename(result['audio_file_path'])
            elif result.get('filename'):
                audio_filename = result['filename']
            
            # Criar URL para acessar o ﾃ｡udio
            audio_url = None
            if audio_filename:
                audio_url = f"/api/automations/audio/{audio_filename}"
            
            tts_result = {
                'audio_file_path': result['audio_file_path'],
                'audio_url': audio_url,
                'filename': audio_filename,
                'duration': result.get('duration', 0),
                'file_size': result.get('file_size', 0),
                'provider_used': provider,
                'voice': voice,
                'speed': speed,
                'emotion': emotion,
                'segments': result.get('segments', []),
                'total_segments': result.get('total_segments', 0),
                'generation_time': datetime.utcnow().isoformat()
            }
            
            self.results['tts'] = tts_result
            
            self._log('info', 'Geraﾃｧﾃ｣o de TTS concluﾃｭda', {
                'provider': provider,
                'duration': result['duration'],
                'file_path': result['audio_file_path']
            })
            
            return tts_result
            
        except Exception as e:
            self._log('error', f'Erro na geraﾃｧﾃ｣o de TTS: {str(e)}')
            raise
    

    
    # ================================
    # 識 ETAPA 7: GERAﾃﾃグ DE IMAGENS
    # ================================
    
    def run_images_generation(self) -> Dict[str, Any]:
        """Executar geraﾃｧﾃ｣o de imagens"""
        try:
            self._log('info', 'Iniciando geraﾃｧﾃ｣o de imagens')
            
            # Verificar se geraﾃｧﾃ｣o de imagens estﾃ｡ habilitada na configuraﾃｧﾃ｣o
            images_config = self.config.get('images', {})
            if not images_config.get('enabled', True):
                self._log('info', 'Geraﾃｧﾃ｣o de imagens desabilitada na configuraﾃｧﾃ｣o, pulando etapa')
                
                # Criar resultado placeholder para compatibilidade
                images_result = {
                    'generated_images': [],
                    'total_images': 0,
                    'provider_used': 'disabled',
                    'style': 'none',
                    'resolution': 'none',
                    'generation_time': datetime.utcnow().isoformat(),
                    'status': 'skipped',
                    'message': 'Geraﾃｧﾃ｣o de imagens desabilitada pelo usuﾃ｡rio'
                }
                
                self.results['images'] = images_result
                self._update_progress('images', 100)
                return images_result
            
            # Verificar se temos roteiro
            if 'scripts' not in self.results:
                raise Exception('Geraﾃｧﾃ｣o de roteiros nﾃ｣o foi executada')
            
            script_data = self.results['scripts']
            script_text = script_data['script']
            
            images_config = self.config.get('images', {})
            provider = images_config.get('provider', 'pollinations')
            style = images_config.get('style', 'cinematic')
            resolution = images_config.get('resolution', '1920x1080')
            per_chapter = images_config.get('per_chapter', 2)  # Manter para compatibilidade
            total_images = images_config.get('total_images', per_chapter * 3)  # Padrﾃ｣o: 6 imagens total
            
            self._update_progress('images', 25)
            
            # Obter prompt personalizado para imagens
            images_prompts = self.custom_prompts.get('images', {})
            custom_image_prompt = images_prompts.get('default', 'Crie uma descriﾃｧﾃ｣o detalhada para geraﾃｧﾃ｣o de imagem baseada no contexto: {context}. A imagem deve ser visualmente atrativa e relevante ao conteﾃｺdo.')
            
            # Importar serviﾃｧo de geraﾃｧﾃ｣o de imagens
            from services.image_generation_service import ImageGenerationService
            
            image_service = ImageGenerationService(self.pipeline_id)
            
            self._update_progress('images', 50)
            
            # Gerar imagens com prompt personalizado usando nova lﾃｳgica de total
            result = image_service.generate_images_for_script_total(
                script_text, provider, style, resolution, total_images, custom_image_prompt
            )
            
            self._update_progress('images', 100)
            
            images_result = {
                'generated_images': result['images'],
                'total_images': len(result['images']),
                'provider_used': provider,
                'style': style,
                'resolution': resolution,
                'generation_time': datetime.utcnow().isoformat()
            }
            
            self.results['images'] = images_result
            
            self._log('info', 'Geraﾃｧﾃ｣o de imagens concluﾃｭda', {
                'total_images': len(result['images']),
                'provider': provider,
                'style': style
            })
            
            return images_result
            
        except Exception as e:
            self._log('error', f'Erro na geraﾃｧﾃ｣o de imagens: {str(e)}')
            raise
    
    # ================================
    # 識 ETAPA 7: CRIAﾃﾃグ DE Vﾃ好EO
    # ================================
    
    def run_video_creation(self) -> Dict[str, Any]:
        """Executar criaﾃｧﾃ｣o do vﾃｭdeo final"""
        try:
            self._log('info', 'Iniciando criaﾃｧﾃ｣o do vﾃｭdeo final')
            
            # Verificar se criaﾃｧﾃ｣o de vﾃｭdeo estﾃ｡ habilitada na configuraﾃｧﾃ｣o
            video_config = self.config.get('video', {})
            if not video_config.get('enabled', True):
                self._log('info', 'Criaﾃｧﾃ｣o de vﾃｭdeo desabilitada na configuraﾃｧﾃ｣o, pulando etapa')
                
                # Criar resultado placeholder para compatibilidade
                video_result = {
                    'video_file_path': None,
                    'duration': 0,
                    'file_size': 0,
                    'resolution': 'none',
                    'fps': 0,
                    'quality': 'none',
                    'transitions': False,
                    'subtitles': False,
                    'creation_time': datetime.utcnow().isoformat(),
                    'status': 'skipped',
                    'message': 'Criaﾃｧﾃ｣o de vﾃｭdeo desabilitada pelo usuﾃ｡rio'
                }
                
                self.results['video'] = video_result
                self._update_progress('video', 100)
                return video_result
            
            # Verificar quais recursos estﾃ｣o disponﾃｭveis baseado nas etapas habilitadas
            available_resources = ['scripts']
            
            # Adicionar TTS se habilitado e executado
            tts_config = self.config.get('tts', {})
            if tts_config.get('enabled', True) and 'tts' in self.results and self.results['tts'].get('status') != 'skipped':
                available_resources.append('tts')
            
            # Adicionar imagens se habilitado e executado
            images_config = self.config.get('images', {})
            if images_config.get('enabled', True) and 'images' in self.results and self.results['images'].get('status') != 'skipped':
                available_resources.append('images')
            
            # Verificar se temos recursos mﾃｭnimos (pelo menos roteiro)
            for req in ['scripts']:
                if req not in self.results:
                    raise Exception(f'Etapa {req} nﾃ｣o foi executada')
            
            video_config = self.config.get('video', {})
            resolution = video_config.get('resolution', '1920x1080')
            fps = video_config.get('fps', 30)
            quality = video_config.get('quality', 'high')
            transitions = video_config.get('transitions', True)
            subtitles = video_config.get('subtitles', True)
            
            self._update_progress('video', 25)
            
            # Criar instﾃ｢ncia do serviﾃｧo de criaﾃｧﾃ｣o de vﾃｭdeo
            video_service = VideoCreationService(self.pipeline_id)
            
            self._update_progress('video', 50)
            
            # Obter segmentos TTS para sincronizaﾃｧﾃ｣o precisa
            tts_segments = self.results['tts'].get('segments', [])
            
            # Criar vﾃｭdeo com sincronizaﾃｧﾃ｣o inteligente
            result = video_service.create_video(
                audio_path=self.results['tts']['audio_file_path'],
                images=self.results['images']['generated_images'],
                script_text=self.results['scripts']['script'],
                resolution=resolution,
                fps=fps,
                quality=quality,
                transitions=transitions,
                subtitles=subtitles,
                tts_segments=tts_segments
            )
            
            self._update_progress('video', 100)
            
            video_result = {
                'video_file_path': result['video_path'],
                'duration': result['duration'],
                'file_size': result['file_size'],
                'resolution': resolution,
                'fps': fps,
                'quality': quality,
                'creation_time': datetime.utcnow().isoformat()
            }
            
            self.results['video'] = video_result
            
            self._log('info', 'Criaﾃｧﾃ｣o de vﾃｭdeo concluﾃｭda', {
                'video_path': result['video_path'],
                'duration': result['duration'],
                'file_size': result['file_size']
            })
            
            return video_result
            
        except Exception as e:
            self._log('error', f'Erro na criaﾃｧﾃ｣o de vﾃｭdeo: {str(e)}')
            # Salvar checkpoint mesmo em caso de erro para permitir retomada
            if self.auto_checkpoint:
                self._save_checkpoint('video_failed')
            raise
    
    # ================================
    # 識 ETAPA 8: LIMPEZA
    # ================================
    
    def run_cleanup(self) -> Dict[str, Any]:
        """Executar limpeza de arquivos temporﾃ｡rios"""
        try:
            self._log('info', 'Iniciando limpeza de arquivos temporﾃ｡rios')
            
            self._update_progress('cleanup', 50)
            
            # Limpar arquivos temporﾃ｡rios (manter apenas o vﾃｭdeo final)
            temp_files = []
            
            # Adicionar arquivos temporﾃ｡rios ﾃ lista de limpeza
            if 'tts' in self.results:
                temp_files.append(self.results['tts']['audio_file_path'])
            
            if 'images' in self.results:
                for image in self.results['images']['generated_images']:
                    if 'temp_path' in image:
                        temp_files.append(image['temp_path'])
            
            # Remover arquivos temporﾃ｡rios
            cleaned_files = []
            for file_path in temp_files:
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                        cleaned_files.append(file_path)
                except Exception as e:
                    self._log('warning', f'Nﾃ｣o foi possﾃｭvel remover arquivo temporﾃ｡rio {file_path}: {str(e)}')
            
            self._update_progress('cleanup', 100)
            
            cleanup_result = {
                'cleaned_files': cleaned_files,
                'files_count': len(cleaned_files),
                'cleanup_time': datetime.utcnow().isoformat()
            }
            
            self.results['cleanup'] = cleanup_result
            
            # Remover checkpoint apﾃｳs conclusﾃ｣o bem-sucedida
            if self.checkpoint_service.has_checkpoint():
                self.checkpoint_service.delete_checkpoint()
                self._log('info', 'Checkpoint removido apﾃｳs conclusﾃ｣o bem-sucedida')
                cleanup_result['checkpoint_removed'] = True
            
            self._log('info', f'Limpeza concluﾃｭda: {len(cleaned_files)} arquivos removidos')
            
            return cleanup_result
            
        except Exception as e:
            self._log('error', f'Erro na limpeza: {str(e)}')
            raise
    
    # ================================
    # 識 VERIFICAﾃﾃグ DE STATUS
    # ================================
    
    def _check_pipeline_status(self) -> bool:
        """Verificar se o pipeline foi pausado ou cancelado"""
        try:
            # Importar aqui para evitar dependﾃｪncia circular
            from routes.pipeline_complete import active_pipelines, PipelineStatus
            
            if self.pipeline_id in active_pipelines:
                status = active_pipelines[self.pipeline_id]['status']
                if status == PipelineStatus.PAUSED:
                    self._log('info', 'Pipeline pausado, aguardando retomada...')
                    self._pause_event.clear()  # Pausar a execuﾃｧﾃ｣o
                    
                    # Aguardar atﾃｩ que o pipeline seja retomado ou cancelado
                    while True:
                        # Verificar status a cada segundo
                        if self._pause_event.wait(timeout=1.0):
                            # Event foi setado, verificar se foi retomado
                            current_status = active_pipelines[self.pipeline_id]['status']
                            if current_status == PipelineStatus.PROCESSING:
                                self._log('info', 'Pipeline retomado, continuando execuﾃｧﾃ｣o...')
                                return True
                            elif current_status == PipelineStatus.CANCELLED:
                                self._log('warning', 'Pipeline cancelado durante pausa')
                                return False
                        else:
                            # Timeout, verificar se foi cancelado
                            current_status = active_pipelines[self.pipeline_id]['status']
                            if current_status == PipelineStatus.CANCELLED:
                                self._log('warning', 'Pipeline cancelado durante pausa')
                                return False
                            elif current_status == PipelineStatus.PROCESSING:
                                self._pause_event.set()  # Sinalizar retomada
                                self._log('info', 'Pipeline retomado, continuando execuﾃｧﾃ｣o...')
                                return True
                elif status == PipelineStatus.CANCELLED:
                    self._log('warning', 'Pipeline cancelado pelo usuﾃ｡rio')
                    return False
            return True
        except Exception as e:
            self._log('warning', f'Erro ao verificar status do pipeline: {str(e)}')
            return True  # Continuar em caso de erro
    
    def _wait_for_resume(self):
        """Aguardar atﾃｩ que o pipeline seja retomado"""
        # Este mﾃｩtodo nﾃ｣o ﾃｩ mais necessﾃ｡rio pois a lﾃｳgica foi movida para _check_pipeline_status
        pass
    
    # ================================
    # 識 EXECUﾃﾃグ COM RETOMADA AUTOMﾃゝICA
    # ================================
    
    def run_with_resume(self, steps: List[str] = None) -> Dict[str, Any]:
        """Executar pipeline com suporte a retomada automﾃ｡tica"""
        try:
            # Log de debug para verificar as etapas carregadas
            pipeline_steps_state = self.pipeline_state.get('steps', {})
            step_names = list(pipeline_steps_state.keys())
            self._log('info', f'DEBUG: Pipeline state carregado com etapas: {step_names}')
            
            # Verificar se script_processing estﾃ｡ presente
            if 'SCRIPT_PROCESSING' in pipeline_steps_state:
                self._log('info', 'DEBUG: Etapa SCRIPT_PROCESSING encontrada no estado')
            else:
                self._log('warning', 'DEBUG: Etapa SCRIPT_PROCESSING Nﾃグ encontrada no estado')
            
            # Se nﾃ｣o foram fornecidas etapas especﾃｭficas, usar as etapas do estado da pipeline
            if steps is None:
                # Obter etapas do estado da pipeline
                pipeline_steps = list(self.pipeline_state.get('steps', {}).keys())
                self._log('info', f'DEBUG: Usando etapas do estado da pipeline: {pipeline_steps}')
                
                # Se nﾃ｣o hﾃ｡ etapas no estado, usar etapas padrﾃ｣o baseadas na configuraﾃｧﾃ｣o
                if not pipeline_steps:
                    self._log('warning', 'DEBUG: Nenhuma etapa encontrada no estado, usando etapas padrﾃ｣o')
                    pipeline_steps = self._get_default_steps()
                    self._log('info', f'DEBUG: Etapas padrﾃ｣o determinadas: {pipeline_steps}')
                
                steps = pipeline_steps
            
            # Verificar se existe checkpoint para retomada
            if self.checkpoint_service.has_checkpoint():
                self._log('info', 'Checkpoint encontrado, retomando pipeline...')
                checkpoint_loaded = self.load_from_checkpoint()
                
                if checkpoint_loaded:
                    # Obter dados do checkpoint para determinar prﾃｳxima etapa
                    checkpoint_data = self.checkpoint_service.load_checkpoint()
                    if checkpoint_data:
                        recovery_report = self.checkpoint_service.create_recovery_report(checkpoint_data)
                        next_step = recovery_report['next_step']
                        
                        if next_step:
                            self._log('info', f'Pipeline retomada a partir da etapa: {next_step}')
                            # Continuar a partir da prﾃｳxima etapa
                            remaining_steps = self._get_remaining_steps(next_step, steps)
                        else:
                            self._log('info', 'Todas as etapas jﾃ｡ foram concluﾃｭdas')
                            remaining_steps = []
                    else:
                        # Se nﾃ｣o conseguir obter dados do checkpoint, comeﾃｧar do inﾃｭcio
                        remaining_steps = steps
                else:
                    # Se nﾃ｣o conseguir carregar checkpoint, comeﾃｧar do inﾃｭcio
                    remaining_steps = steps
            else:
                # Executar pipeline completa
                remaining_steps = steps
            
            # Executar etapas restantes
            for step in remaining_steps:
                # Verificar se pipeline foi pausado ou cancelado antes de cada etapa
                if not self._check_pipeline_status():
                    self._log('info', f'Pipeline pausado/cancelado antes da etapa: {step}')
                    return self.results
                
                try:
                    self._log('info', f'Executando etapa: {step}')
                    
                    if step == 'extraction':
                        self.run_extraction()
                    elif step == 'titles':
                        self.run_titles_generation()
                    elif step == 'premises':
                        self.run_premises_generation()
                    elif step == 'scripts':
                        self.run_scripts_generation()
                    elif step == 'script_processing':
                        self.run_script_processing()
                    elif step == 'tts':
                        self.run_tts_generation()
                    elif step == 'images':
                        self.run_images_generation()
                    elif step == 'video':
                        self.run_video_creation()
                    elif step == 'cleanup':
                        self.run_cleanup()
                    else:
                        self._log('warning', f'Etapa desconhecida: {step}')
                        continue
                    
                    # Verificar novamente apﾃｳs a execuﾃｧﾃ｣o da etapa
                    if not self._check_pipeline_status():
                        self._log('info', f'Pipeline pausado/cancelado apﾃｳs a etapa: {step}')
                        return self.results
                    
                    # Salvar checkpoint apﾃｳs cada etapa bem-sucedida
                    if self.auto_checkpoint and step != 'cleanup':
                        self._save_checkpoint(step)
                    
                except Exception as e:
                    self._log('error', f'Erro na etapa {step}: {str(e)}')
                    # Salvar checkpoint mesmo em caso de erro
                    if self.auto_checkpoint:
                        self._save_checkpoint(f'{step}_failed')
                    raise
            
            self._log('info', 'Pipeline executada com sucesso!')
            self._log('info', f'Resultados finais: {list(self.results.keys())}')
            
            # Debug: verificar conteﾃｺdo dos resultados
            for step, result in self.results.items():
                if isinstance(result, dict):
                    self._log('info', f'Step {step}: {len(str(result))} chars de dados')
                else:
                    self._log('info', f'Step {step}: tipo {type(result)}')
            
            return self.results
            
        except Exception as e:
            self._log('error', f'Erro na execuﾃｧﾃ｣o da pipeline: {str(e)}')
            raise
    
    def _get_default_steps(self) -> List[str]:
        """Obter lista padrﾃ｣o de etapas da pipeline baseada na configuraﾃｧﾃ｣o"""
        enabled_steps = []
        
        # Log da configuraﾃｧﾃ｣o para debug
        self._log('info', f'Configuraﾃｧﾃ｣o da pipeline: {json.dumps(self.config, indent=2)}')
        
        # Verificar quais etapas estﾃ｣o habilitadas
        if self.config.get('extraction', {}).get('enabled', True):
            enabled_steps.append('extraction')
        if self.config.get('titles', {}).get('enabled', True):
            enabled_steps.append('titles')
        if self.config.get('premises', {}).get('enabled', True):
            enabled_steps.append('premises')
        if self.config.get('scripts', {}).get('enabled', True):
            enabled_steps.append('scripts')
        
        # Verificar script_processing especificamente
        script_processing_config = self.config.get('script_processing', {})
        script_processing_enabled = script_processing_config.get('enabled', True)
        self._log('info', f'Script processing config: {script_processing_config}')
        self._log('info', f'Script processing enabled: {script_processing_enabled}')
        
        if script_processing_enabled:
            enabled_steps.append('script_processing')
            
        if self.config.get('tts', {}).get('enabled', True):
            enabled_steps.append('tts')
        if self.config.get('images', {}).get('enabled', True):
            enabled_steps.append('images')
        if self.config.get('video', {}).get('enabled', True):
            enabled_steps.append('video')
        
        # Cleanup sempre habilitado se hﾃ｡ pelo menos uma etapa
        if enabled_steps:
            enabled_steps.append('cleanup')
        
        self._log('info', f'Etapas habilitadas: {enabled_steps}')
        return enabled_steps
    
    def _get_remaining_steps(self, next_step: str, all_steps: List[str] = None) -> List[str]:
        """Obter etapas restantes a partir de uma etapa especﾃｭfica"""
        if not all_steps:
            # Usar etapas do estado da pipeline em vez de _get_default_steps
            all_steps = list(self.pipeline_state.get('steps', {}).keys())
            self._log('info', f'Usando etapas do estado para remaining_steps: {all_steps}')
        
        try:
            start_index = all_steps.index(next_step)
            remaining = all_steps[start_index:]
            self._log('info', f'Etapas restantes a partir de {next_step}: {remaining}')
            return remaining
        except ValueError:
            # Se a etapa nﾃ｣o for encontrada, executar todas
            self._log('info', f'Etapa {next_step} nﾃ｣o encontrada, executando todas: {all_steps}')
            return all_steps
# üîß Processos Detalhados da Pipeline - Auto Video Producer

## üìã √çndice

1. [Processo de Extra√ß√£o](#1-processo-de-extra√ß√£o)
2. [Processo de Gera√ß√£o de T√≠tulos](#2-processo-de-gera√ß√£o-de-t√≠tulos)
3. [Processo de Gera√ß√£o de Premissa](#3-processo-de-gera√ß√£o-de-premissa)
4. [Processo de Gera√ß√£o de Roteiro](#4-processo-de-gera√ß√£o-de-roteiro)
5. [Processo de Gera√ß√£o de √Åudio (TTS)](#5-processo-de-gera√ß√£o-de-√°udio-tts)
6. [Processo de Gera√ß√£o de Imagens](#6-processo-de-gera√ß√£o-de-imagens)
7. [Processo de Cria√ß√£o de V√≠deo](#7-processo-de-cria√ß√£o-de-v√≠deo)
8. [Processo de Orquestra√ß√£o](#8-processo-de-orquestra√ß√£o)

---

## 1. üì∫ Processo de Extra√ß√£o

### Localiza√ß√£o
- **Arquivo**: `backend/routes/automations.py`
- **Fun√ß√£o**: `extract_youtube_data()`
- **Endpoint**: `POST /api/extract-youtube`

### Depend√™ncias T√©cnicas
```python
import yt_dlp
import requests
import json
import re
from urllib.parse import urlparse
```

### Fluxo Detalhado

#### Etapa 1: Valida√ß√£o da URL
```python
def validate_youtube_url(url):
    patterns = [
        r'youtube\.com/channel/([a-zA-Z0-9_-]+)',
        r'youtube\.com/c/([a-zA-Z0-9_-]+)',
        r'youtube\.com/@([a-zA-Z0-9_-]+)',
        r'youtube\.com/user/([a-zA-Z0-9_-]+)'
    ]
    # Valida formato e extrai ID do canal
```

#### Etapa 2: Configura√ß√£o do yt-dlp
```python
ydl_opts = {
    'quiet': True,
    'no_warnings': True,
    'extract_flat': True,
    'playlist_items': '1:50',  # Limita a 50 v√≠deos
    'ignoreerrors': True
}
```

#### Etapa 3: Extra√ß√£o de Metadados
```python
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(channel_url, download=False)
    
    # Extrai informa√ß√µes do canal
    channel_info = {
        'name': info.get('uploader', ''),
        'id': info.get('channel_id', ''),
        'subscriber_count': info.get('subscriber_count', 0),
        'video_count': info.get('playlist_count', 0)
    }
    
    # Extrai t√≠tulos dos v√≠deos
    video_titles = []
    for entry in info.get('entries', []):
        if entry and entry.get('title'):
            video_titles.append({
                'title': entry['title'],
                'view_count': entry.get('view_count', 0),
                'upload_date': entry.get('upload_date', ''),
                'duration': entry.get('duration', 0)
            })
```

#### Etapa 4: An√°lise de Padr√µes
```python
def analyze_viral_patterns(titles):
    patterns = {
        'emotional_triggers': [],
        'numbers': [],
        'power_words': [],
        'length_stats': {}
    }
    
    for title in titles:
        # Detecta gatilhos emocionais
        emotional_words = ['incr√≠vel', 'chocante', 'segredo', 'revelado']
        
        # Detecta n√∫meros
        numbers = re.findall(r'\d+', title)
        
        # Calcula estat√≠sticas de comprimento
        # ...
    
    return patterns
```

### Estrutura de Retorno
```json
{
    "success": true,
    "data": {
        "channel_info": {
            "name": "Nome do Canal",
            "id": "UCxxxxx",
            "subscriber_count": 100000,
            "video_count": 250
        },
        "video_titles": [
            {
                "title": "T√≠tulo do V√≠deo",
                "view_count": 50000,
                "upload_date": "20240115",
                "duration": 600
            }
        ],
        "viral_patterns": {
            "emotional_triggers": ["incr√≠vel", "segredo"],
            "numbers": ["10", "2024"],
            "power_words": ["como", "melhor"],
            "length_stats": {
                "avg_length": 45,
                "min_length": 20,
                "max_length": 80
            }
        }
    }
}
```

### Tratamento de Erros
- **URL inv√°lida**: Retorna erro 400 com mensagem espec√≠fica
- **Canal n√£o encontrado**: Retorna erro 404
- **Rate limiting**: Implementa retry com backoff exponencial
- **Timeout**: Configura timeout de 30 segundos

---

## 2. üéØ Processo de Gera√ß√£o de T√≠tulos

### Localiza√ß√£o
- **Arquivo**: `backend/services/title_generator.py`
- **Classe**: `TitleGenerator`
- **Endpoint**: `POST /api/generate-titles`

### Provedores Suportados

#### OpenAI
```python
def configure_openai(self, api_key):
    from openai import OpenAI
    self.openai_client = OpenAI(api_key=api_key)
    
    # Teste de conex√£o
    models = self.openai_client.models.list()
```

#### Google Gemini
```python
def configure_gemini(self, api_key):
    import google.generativeai as genai
    genai.configure(api_key=api_key)
    self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
```

#### OpenRouter
```python
def configure_openrouter(self, api_key):
    self.openrouter_api_key = api_key
    # Suporta m√∫ltiplos modelos: Claude, Llama, etc.
```

### An√°lise de Padr√µes Virais

```python
def analyze_viral_patterns(self, titles):
    patterns = {
        'emotional_triggers': [],
        'numbers': [],
        'power_words': [],
        'structures': [],
        'length_stats': {}
    }
    
    # Gatilhos emocionais
    emotional_words = [
        'incr√≠vel', 'chocante', 'segredo', 'revelado',
        'nunca', 'sempre', 'todos', 'ningu√©m',
        'melhor', 'pior', '√∫nico', 'especial'
    ]
    
    # Palavras de poder
    power_words = [
        'como', 'por que', 'quando', 'onde',
        'dicas', 'truques', 'hacks', 'segredos',
        'm√©todo', 't√©cnica', 'estrat√©gia'
    ]
    
    # Estruturas comuns
    structures = [
        r'^\d+\s+\w+',  # "10 Dicas"
        r'Como\s+\w+',  # "Como Fazer"
        r'Por que\s+\w+',  # "Por que Voc√™"
        r'O que\s+\w+'   # "O que Acontece"
    ]
    
    for title in titles:
        # An√°lise de cada padr√£o
        # ...
    
    return patterns
```

### Gera√ß√£o de Prompts

#### Prompt para OpenAI
```python
def create_openai_prompt(self, source_titles, topic, patterns, style, count):
    prompt = f"""
    Voc√™ √© um especialista em cria√ß√£o de t√≠tulos virais para YouTube.
    
    T√çTULOS DE REFER√äNCIA:
    {chr(10).join(source_titles[:10])}
    
    PADR√ïES IDENTIFICADOS:
    - Gatilhos emocionais: {', '.join(patterns['emotional_triggers'][:5])}
    - Palavras de poder: {', '.join(patterns['power_words'][:5])}
    - Comprimento m√©dio: {patterns['length_stats'].get('avg', 50)} caracteres
    
    T√ìPICO: {topic}
    ESTILO: {style}
    
    Crie {count} t√≠tulos √∫nicos e virais seguindo os padr√µes identificados.
    
    REGRAS:
    1. Use gatilhos emocionais
    2. Inclua n√∫meros quando apropriado
    3. Mantenha entre 40-60 caracteres
    4. Seja espec√≠fico e intrigante
    5. Evite clickbait enganoso
    
    FORMATO: Liste apenas os t√≠tulos, um por linha.
    """
    return prompt
```

#### Prompt para Gemini
```python
def create_gemini_prompt(self, source_titles, topic, patterns, style, count):
    prompt = f"""
    ## Contexto
    Voc√™ √© um especialista em marketing digital e cria√ß√£o de conte√∫do viral.
    
    ## Dados de Entrada
    **T√≠tulos de Refer√™ncia:**
    {chr(10).join([f"- {title}" for title in source_titles[:10]])}
    
    **Padr√µes Virais Identificados:**
    - Gatilhos emocionais mais usados: {', '.join(patterns['emotional_triggers'][:5])}
    - Palavras de poder: {', '.join(patterns['power_words'][:5])}
    - Estruturas comuns: {', '.join(patterns['structures'][:3])}
    
    ## Tarefa
    Crie {count} t√≠tulos √∫nicos para o t√≥pico: "{topic}"
    Estilo desejado: {style}
    
    ## Diretrizes
    1. **Emo√ß√£o**: Use gatilhos emocionais identificados
    2. **Clareza**: Seja espec√≠fico sobre o benef√≠cio
    3. **Urg√™ncia**: Crie senso de urg√™ncia quando apropriado
    4. **Curiosidade**: Desperte curiosidade sem ser clickbait
    5. **Comprimento**: Entre 40-60 caracteres idealmente
    
    ## Formato de Sa√≠da
    Retorne apenas os t√≠tulos, um por linha, sem numera√ß√£o.
    """
    return prompt
```

### Processamento de Respostas

```python
def parse_generated_titles(self, content):
    # Remove numera√ß√£o e formata√ß√£o
    lines = content.strip().split('\n')
    titles = []
    
    for line in lines:
        # Remove numera√ß√£o (1., 2., etc.)
        clean_line = re.sub(r'^\d+\.?\s*', '', line.strip())
        # Remove aspas
        clean_line = clean_line.strip('"\'')
        # Remove marcadores
        clean_line = re.sub(r'^[-*]\s*', '', clean_line)
        
        if clean_line and len(clean_line) > 10:
            titles.append(clean_line)
    
    return titles
```

### Sistema de Pontua√ß√£o

```python
def score_title_quality(self, title, patterns):
    score = 0
    
    # Comprimento ideal (40-60 caracteres)
    length = len(title)
    if 40 <= length <= 60:
        score += 20
    elif 30 <= length <= 70:
        score += 10
    
    # Presen√ßa de gatilhos emocionais
    for trigger in patterns['emotional_triggers']:
        if trigger.lower() in title.lower():
            score += 15
            break
    
    # Presen√ßa de n√∫meros
    if re.search(r'\d+', title):
        score += 10
    
    # Palavras de poder
    for word in patterns['power_words']:
        if word.lower() in title.lower():
            score += 10
            break
    
    # Estruturas virais
    for structure in patterns['structures']:
        if re.search(structure, title, re.IGNORECASE):
            score += 15
            break
    
    # Penalidades
    if len(title) > 80:  # Muito longo
        score -= 20
    if len(title) < 20:  # Muito curto
        score -= 15
    
    return min(100, max(0, score))
```

---

## 3. üí° Processo de Gera√ß√£o de Premissa

### Localiza√ß√£o
- **Arquivo**: `backend/routes/automations.py`
- **Fun√ß√£o**: `generate_premise()`
- **Endpoint**: `POST /api/generate-premise`

### Fluxo de Gera√ß√£o

```python
def generate_premise():
    data = request.get_json()
    title = data.get('title')
    channel_context = data.get('channel_context', {})
    ai_provider = data.get('ai_provider', 'gemini')
    
    # Construir contexto
    context = build_premise_context(title, channel_context)
    
    # Gerar prompt
    prompt = create_premise_prompt(title, context)
    
    # Chamar IA
    premise = call_ai_for_premise(prompt, ai_provider)
    
    # Validar e formatar
    formatted_premise = format_premise(premise)
    
    return formatted_premise
```

### Constru√ß√£o de Contexto

```python
def build_premise_context(title, channel_context):
    context = {
        'channel_name': channel_context.get('name', ''),
        'channel_niche': analyze_channel_niche(channel_context),
        'target_audience': infer_target_audience(channel_context),
        'content_style': analyze_content_style(channel_context)
    }
    
    # An√°lise do t√≠tulo
    title_analysis = {
        'main_topic': extract_main_topic(title),
        'emotional_hook': identify_emotional_hook(title),
        'target_benefit': identify_target_benefit(title)
    }
    
    context['title_analysis'] = title_analysis
    return context
```

### Template de Prompt

```python
def create_premise_prompt(title, context):
    prompt = f"""
    ## Contexto do Canal
    - Nome: {context['channel_name']}
    - Nicho: {context['channel_niche']}
    - P√∫blico-alvo: {context['target_audience']}
    - Estilo: {context['content_style']}
    
    ## T√≠tulo do V√≠deo
    "{title}"
    
    ## An√°lise do T√≠tulo
    - T√≥pico principal: {context['title_analysis']['main_topic']}
    - Gancho emocional: {context['title_analysis']['emotional_hook']}
    - Benef√≠cio prometido: {context['title_analysis']['target_benefit']}
    
    ## Tarefa
    Crie uma premissa envolvente para este v√≠deo que:
    
    1. **Estabele√ßa o problema/oportunidade** que o v√≠deo vai abordar
    2. **Prometa uma solu√ß√£o espec√≠fica** alinhada com o t√≠tulo
    3. **Crie curiosidade** sobre como a solu√ß√£o ser√° revelada
    4. **Seja relevante** para o p√∫blico-alvo do canal
    5. **Mantenha coer√™ncia** com o estilo do canal
    
    ## Estrutura Desejada
    - **Hook inicial**: Frase que captura aten√ß√£o imediatamente
    - **Problema/Oportunidade**: O que ser√° abordado
    - **Promessa**: O que o espectador vai aprender/ganhar
    - **Curiosidade**: Como isso ser√° revelado no v√≠deo
    
    ## Formato
    Retorne apenas a premissa, sem explica√ß√µes adicionais.
    M√°ximo 200 palavras.
    """
    return prompt
```

### Valida√ß√£o da Premissa

```python
def validate_premise(premise, title):
    validation = {
        'length_ok': 50 <= len(premise) <= 1000,
        'has_hook': check_for_hook(premise),
        'aligns_with_title': check_title_alignment(premise, title),
        'has_promise': check_for_promise(premise),
        'creates_curiosity': check_for_curiosity(premise)
    }
    
    validation['score'] = sum(validation.values()) / len(validation) * 100
    return validation
```

---

## 4. üìù Processo de Gera√ß√£o de Roteiro

### Localiza√ß√£o
- **Arquivo**: `backend/services/storyteller_service.py`
- **Classe**: `StorytellerService`
- **Endpoint**: `POST /api/generate-script`

### Pipeline de 3 Prompts

#### Prompt 1: Estrutura Inicial
```python
def execute_prompt_1(self, title, premise, agent_type, target_chars):
    prompt = f"""
    ## Contexto
    Voc√™ √© um {self.agent_configs[agent_type]['description']}
    
    ## Informa√ß√µes do V√≠deo
    **T√≠tulo:** {title}
    **Premissa:** {premise}
    **Dura√ß√£o alvo:** {target_chars} caracteres
    
    ## Tarefa - Prompt 1: Estrutura Inicial
    Crie a estrutura base do roteiro com:
    
    1. **Abertura impactante** (10% do conte√∫do)
       - Hook que prende aten√ß√£o nos primeiros 15 segundos
       - Apresenta√ß√£o clara do que ser√° abordado
    
    2. **Desenvolvimento principal** (70% do conte√∫do)
       - Divis√£o em 3-5 pontos principais
       - Cada ponto com explica√ß√£o e exemplo
       - Transi√ß√µes suaves entre pontos
    
    3. **Fechamento forte** (20% do conte√∫do)
       - Resumo dos pontos principais
       - Call-to-action claro
       - Gancho para pr√≥ximo v√≠deo (se aplic√°vel)
    
    ## Diretrizes
    - Use linguagem {self.agent_configs[agent_type]['tone']}
    - Mantenha {self.agent_configs[agent_type]['style']}
    - Inclua momentos de intera√ß√£o com o p√∫blico
    - Evite repeti√ß√µes desnecess√°rias
    
    ## Formato
    Retorne o roteiro estruturado com marca√ß√µes de se√ß√£o.
    """
    
    return self._call_llm_api(prompt, self._get_next_gemini_key(), 'gemini')
```

#### Prompt 2: Desenvolvimento e Detalhamento
```python
def execute_prompt_2(self, prompt_1_result, title, premise, agent_type):
    prompt = f"""
    ## Roteiro Base (Prompt 1)
    {prompt_1_result}
    
    ## Tarefa - Prompt 2: Desenvolvimento e Detalhamento
    Expanda o roteiro base adicionando:
    
    1. **Detalhamento de cada se√ß√£o**
       - Adicione exemplos espec√≠ficos
       - Inclua dados e estat√≠sticas relevantes
       - Desenvolva analogias e met√°foras
    
    2. **Elementos de engajamento**
       - Perguntas ret√≥ricas estrat√©gicas
       - Momentos de pausa para reflex√£o
       - Chamadas para intera√ß√£o
    
    3. **Narrativa fluida**
       - Conectores entre ideias
       - Varia√ß√£o no ritmo da narra√ß√£o
       - Momentos de tens√£o e al√≠vio
    
    4. **Personaliza√ß√£o**
       - Hist√≥rias pessoais ou casos reais
       - Refer√™ncias culturais apropriadas
       - Linguagem do p√∫blico-alvo
    
    ## Foco Especial
    - Mantenha coer√™ncia com o t√≠tulo: "{title}"
    - Cumpra a promessa da premissa: "{premise}"
    - Use o estilo {agent_type}
    
    ## Formato
    Retorne o roteiro expandido mantendo a estrutura original.
    """
    
    return self._call_llm_api(prompt, self._get_next_gemini_key(), 'gemini')
```

#### Prompt 3: Refinamento e Polimento
```python
def execute_prompt_3(self, prompt_2_result, title, premise, agent_type):
    prompt = f"""
    ## Roteiro Desenvolvido (Prompt 2)
    {prompt_2_result}
    
    ## Tarefa - Prompt 3: Refinamento e Polimento
    Finalize o roteiro com foco em:
    
    1. **Otimiza√ß√£o da linguagem**
       - Simplifique frases complexas
       - Elimine redund√¢ncias
       - Melhore a fluidez da narra√ß√£o
    
    2. **Timing e ritmo**
       - Ajuste pausas estrat√©gicas
       - Varie o ritmo conforme o conte√∫do
       - Otimize para reten√ß√£o de audi√™ncia
    
    3. **Impacto emocional**
       - Intensifique momentos-chave
       - Adicione elementos de surpresa
       - Reforce o valor entregue
    
    4. **Chamadas para a√ß√£o**
       - Posicione CTAs estrategicamente
       - Torne-as naturais e persuasivas
       - Varie o tipo de intera√ß√£o solicitada
    
    5. **Verifica√ß√£o final**
       - Confirme alinhamento com t√≠tulo
       - Valide cumprimento da premissa
       - Assegure qualidade do conte√∫do
    
    ## Resultado Final
    Entregue um roteiro polido, envolvente e pronto para produ√ß√£o.
    """
    
    return self._call_llm_api(prompt, self._get_next_gemini_key(), 'gemini')
```

### Detec√ß√£o de Repeti√ß√µes

```python
class RepetitionDetector:
    def __init__(self):
        self.similarity_threshold = 0.7
        self.phrase_min_length = 10
    
    def detect_repetitions(self, chapters):
        repetitions = []
        
        # Detecta cap√≠tulos similares
        for i, chapter1 in enumerate(chapters):
            for j, chapter2 in enumerate(chapters[i+1:], i+1):
                similarity = self._calculate_similarity(chapter1, chapter2)
                if similarity > self.similarity_threshold:
                    repetitions.append({
                        'chapter1': i+1,
                        'chapter2': j+1,
                        'similarity': similarity
                    })
        
        # Detecta frases repetidas
        repeated_phrases = self._find_repeated_phrases(chapters)
        
        return {
            'similar_chapters': repetitions,
            'repeated_phrases': repeated_phrases,
            'repetition_score': len(repetitions) + len(repeated_phrases)
        }
    
    def _calculate_similarity(self, text1, text2):
        from difflib import SequenceMatcher
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def _find_repeated_phrases(self, chapters):
        phrases = []
        all_sentences = []
        
        # Extrai senten√ßas
        for i, chapter in enumerate(chapters):
            sentences = re.split(r'[.!?]+', chapter)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) >= self.phrase_min_length:
                    all_sentences.append({
                        'text': sentence.lower(),
                        'chapter': i+1,
                        'original': sentence
                    })
        
        # Encontra duplicatas
        seen = {}
        for sentence in all_sentences:
            text = sentence['text']
            if text in seen:
                phrases.append({
                    'phrase': sentence['original'],
                    'chapters': [seen[text]['chapter'], sentence['chapter']]
                })
            else:
                seen[text] = sentence
        
        return phrases
```

### Quebra Inteligente de Cap√≠tulos

```python
class SmartChapterBreaker:
    def __init__(self):
        self.break_indicators = [
            r'\.\s*\n+',  # Fim de par√°grafo
            r'\n\s*\n',   # Linha em branco
            r'[.!?]\s+[A-Z]',  # Fim de frase + nova frase
            r'\b(Agora|Ent√£o|Portanto|Al√©m disso|Por outro lado)\b',
            r'\b(Primeiro|Segundo|Terceiro|Finalmente)\b',
            r'\b(Vamos|Agora vamos|Pr√≥ximo)\b'
        ]
    
    def find_natural_breaks(self, text, target_length):
        # Encontra pontos naturais de quebra
        break_points = []
        
        for pattern in self.break_indicators:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                break_points.append(match.start())
        
        # Ordena e remove duplicatas
        break_points = sorted(set(break_points))
        
        # Seleciona pontos pr√≥ximos ao comprimento alvo
        optimal_breaks = []
        current_pos = 0
        
        for target_pos in range(target_length, len(text), target_length):
            # Encontra o ponto de quebra mais pr√≥ximo
            best_break = min(break_points, 
                           key=lambda x: abs(x - target_pos) if x > current_pos else float('inf'))
            
            if best_break > current_pos:
                optimal_breaks.append(best_break)
                current_pos = best_break
        
        return optimal_breaks
```

### Valida√ß√£o de Qualidade

```python
class StoryValidator:
    def __init__(self, config):
        self.config = config
        self.repetition_detector = RepetitionDetector()
    
    def validate_chapter(self, chapter, chapter_num):
        validation = {
            'length_ok': self._check_length(chapter),
            'has_hook': self._check_hook(chapter, chapter_num),
            'has_content': self._check_content_quality(chapter),
            'has_transition': self._check_transitions(chapter),
            'language_quality': self._check_language(chapter)
        }
        
        validation['score'] = sum(validation.values()) / len(validation) * 100
        return validation
    
    def _check_length(self, chapter):
        min_chars = self.config.get('min_chars', 500)
        max_chars = self.config.get('max_chars', 2000)
        return min_chars <= len(chapter) <= max_chars
    
    def _check_hook(self, chapter, chapter_num):
        if chapter_num == 1:  # Primeiro cap√≠tulo deve ter hook forte
            hook_patterns = [
                r'^(Voc√™ j√°|Imagine|E se|Sabia que)',
                r'\?',  # Pergunta
                r'(incr√≠vel|surpreendente|chocante)'
            ]
            return any(re.search(pattern, chapter[:200], re.IGNORECASE) 
                      for pattern in hook_patterns)
        return True
    
    def _check_content_quality(self, chapter):
        # Verifica se tem conte√∫do substantivo
        sentences = re.split(r'[.!?]+', chapter)
        substantial_sentences = [s for s in sentences if len(s.strip()) > 20]
        return len(substantial_sentences) >= 3
    
    def _check_transitions(self, chapter):
        transition_words = [
            'al√©m disso', 'portanto', 'ent√£o', 'agora',
            'por outro lado', 'em seguida', 'finalmente'
        ]
        return any(word in chapter.lower() for word in transition_words)
    
    def _check_language(self, chapter):
        # Verifica qualidade b√°sica da linguagem
        issues = 0
        
        # Frases muito longas
        sentences = re.split(r'[.!?]+', chapter)
        long_sentences = [s for s in sentences if len(s.split()) > 30]
        if len(long_sentences) > len(sentences) * 0.3:
            issues += 1
        
        # Repeti√ß√µes de palavras
        words = re.findall(r'\b\w+\b', chapter.lower())
        word_freq = {}
        for word in words:
            if len(word) > 4:  # Ignora palavras muito curtas
                word_freq[word] = word_freq.get(word, 0) + 1
        
        repeated_words = [w for w, freq in word_freq.items() if freq > 5]
        if len(repeated_words) > 3:
            issues += 1
        
        return issues == 0
```

---

## 5. üéµ Processo de Gera√ß√£o de √Åudio (TTS)

### Localiza√ß√£o
- **Arquivo**: `backend/services/tts_service.py`
- **Classe**: `TTSService`
- **Endpoint**: `POST /api/generate-tts`

### Provedores Suportados

#### Google Gemini TTS
```python
def generate_tts_with_gemini(text, voice_settings):
    import google.generativeai as genai
    
    # Configura√ß√£o do modelo
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    # Par√¢metros de voz
    voice_config = {
        'voice_name': voice_settings.get('voice_id', 'pt-BR-Standard-A'),
        'language_code': 'pt-BR',
        'speaking_rate': voice_settings.get('speed', 1.0),
        'pitch': voice_settings.get('pitch', 0.0),
        'volume_gain_db': voice_settings.get('volume', 0.0)
    }
    
    # Gerar √°udio
    response = model.generate_content(
        text,
        generation_config={
            'audio_config': voice_config
        }
    )
    
    return response.audio_data
```

#### ElevenLabs
```python
def generate_tts_with_elevenlabs(text, voice_settings):
    import requests
    
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_settings['voice_id']}"
    
    headers = {
        "Accept": "audio/mpeg",
        "Content-Type": "application/json",
        "xi-api-key": voice_settings['api_key']
    }
    
    data = {
        "text": text,
        "model_id": voice_settings.get('model_id', 'eleven_monolingual_v1'),
        "voice_settings": {
            "stability": voice_settings.get('stability', 0.5),
            "similarity_boost": voice_settings.get('clarity', 0.75),
            "style": voice_settings.get('style', 0.0),
            "use_speaker_boost": voice_settings.get('speaker_boost', True)
        }
    }
    
    response = requests.post(url, json=data, headers=headers)
    return response.content
```

#### Kokoro TTS
```python
def generate_tts_with_kokoro(text, voice_settings):
    import requests
    
    # Kokoro √© um modelo local/open-source
    url = voice_settings.get('kokoro_url', 'http://localhost:8080/tts')
    
    data = {
        'text': text,
        'voice': voice_settings.get('voice_id', 'kokoro'),
        'speed': voice_settings.get('speed', 1.0),
        'pitch': voice_settings.get('pitch', 0.0)
    }
    
    response = requests.post(url, json=data)
    return response.content
```

### Segmenta√ß√£o Inteligente

```python
def _segment_text(self, text, max_chars_per_segment):
    # Divide texto em segmentos respeitando limites naturais
    segments = []
    current_segment = ""
    
    # Divide por senten√ßas primeiro
    sentences = re.split(r'[.!?]+', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # Se adicionar esta senten√ßa ultrapassar o limite
        if len(current_segment + sentence) > max_chars_per_segment:
            if current_segment:  # Se j√° tem conte√∫do, salva o segmento
                segments.append(current_segment.strip())
                current_segment = sentence
            else:  # Senten√ßa muito longa, divide por palavras
                words = sentence.split()
                for word in words:
                    if len(current_segment + " " + word) > max_chars_per_segment:
                        if current_segment:
                            segments.append(current_segment.strip())
                            current_segment = word
                        else:
                            # Palavra muito longa, for√ßa divis√£o
                            segments.append(word)
                    else:
                        current_segment += " " + word if current_segment else word
        else:
            current_segment += ". " + sentence if current_segment else sentence
    
    # Adiciona √∫ltimo segmento
    if current_segment:
        segments.append(current_segment.strip())
    
    return segments
```

### Sincroniza√ß√£o e Timing

```python
def _calculate_timing(self, text_segments, audio_files):
    timing_data = []
    cumulative_time = 0
    
    for i, (text, audio_file) in enumerate(zip(text_segments, audio_files)):
        # Calcula dura√ß√£o do √°udio
        duration = self._get_audio_duration(audio_file)
        
        # Calcula palavras por segundo
        word_count = len(text.split())
        wps = word_count / duration if duration > 0 else 0
        
        timing_data.append({
            'segment_index': i,
            'text': text,
            'audio_file': audio_file,
            'start_time': cumulative_time,
            'end_time': cumulative_time + duration,
            'duration': duration,
            'word_count': word_count,
            'words_per_second': wps
        })
        
        cumulative_time += duration
    
    return timing_data

def _get_audio_duration(self, audio_file):
    try:
        from moviepy.editor import AudioFileClip
        with AudioFileClip(audio_file) as audio:
            return audio.duration
    except Exception as e:
        # Fallback usando ffprobe
        import subprocess
        result = subprocess.run([
            'ffprobe', '-v', 'quiet', '-show_entries',
            'format=duration', '-of', 'csv=p=0', audio_file
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            return float(result.stdout.strip())
        return 0
```

### Combina√ß√£o de Segmentos

```python
def _combine_audio_segments(self, audio_files, output_path):
    try:
        from moviepy.editor import AudioFileClip, concatenate_audioclips
        
        # Carrega todos os clipes de √°udio
        audio_clips = []
        for audio_file in audio_files:
            if os.path.exists(audio_file):
                clip = AudioFileClip(audio_file)
                audio_clips.append(clip)
        
        if not audio_clips:
            raise Exception("Nenhum arquivo de √°udio v√°lido encontrado")
        
        # Concatena todos os clipes
        final_audio = concatenate_audioclips(audio_clips)
        
        # Salva o √°udio final
        final_audio.write_audiofile(output_path, verbose=False, logger=None)
        
        # Limpa recursos
        for clip in audio_clips:
            clip.close()
        final_audio.close()
        
        return output_path
        
    except Exception as e:
        # Fallback usando ffmpeg
        return self._combine_with_ffmpeg(audio_files, output_path)

def _combine_with_ffmpeg(self, audio_files, output_path):
    import subprocess
    
    # Cria lista de arquivos para ffmpeg
    file_list_path = output_path + '.txt'
    with open(file_list_path, 'w') as f:
        for audio_file in audio_files:
            if os.path.exists(audio_file):
                f.write(f"file '{audio_file}'\n")
    
    # Executa ffmpeg
    cmd = [
        'ffmpeg', '-f', 'concat', '-safe', '0',
        '-i', file_list_path, '-c', 'copy', output_path, '-y'
    ]
    
    result = subprocess.run(cmd, capture_output=True)
    
    # Remove arquivo tempor√°rio
    os.remove(file_list_path)
    
    if result.returncode == 0:
        return output_path
    else:
        raise Exception(f"Erro no ffmpeg: {result.stderr.decode()}")
```

---

## 6. üé® Processo de Gera√ß√£o de Imagens

### Localiza√ß√£o
- **Arquivo**: `backend/routes/images.py`
- **Blueprint**: `images_bp`
- **Endpoint**: `POST /api/images/generate`

### Provedores Suportados

#### Together AI
```python
def generate_image_together(prompt, model, style_settings):
    import requests
    
    url = "https://api.together.xyz/v1/images/generations"
    
    headers = {
        "Authorization": f"Bearer {style_settings['api_key']}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,  # ex: "black-forest-labs/FLUX.1-schnell"
        "prompt": prompt,
        "width": style_settings.get('width', 1024),
        "height": style_settings.get('height', 1024),
        "steps": style_settings.get('steps', 20),
        "n": 1,
        "response_format": "b64_json"
    }
    
    response = requests.post(url, headers=headers, json=data)
    
    if response.status_code == 200:
        result = response.json()
        return result['data'][0]['b64_json']
    else:
        raise Exception(f"Erro Together AI: {response.text}")
```

#### Google Gemini (Imagen)
```python
def generate_image_gemini(prompt, style_settings):
    import google.generativeai as genai
    
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    # Gemini usa prompt textual para gerar imagem
    image_prompt = f"""
    Gere uma imagem baseada na seguinte descri√ß√£o:
    {prompt}
    
    Estilo: {style_settings.get('style', 'realistic')}
    Qualidade: {style_settings.get('quality', 'high')}
    Formato: {style_settings.get('format', 'landscape')}
    """
    
    response = model.generate_content(
        image_prompt,
        generation_config={
            'image_config': {
                'width': style_settings.get('width', 1024),
                'height': style_settings.get('height', 1024)
            }
        }
    )
    
    return response.image_data
```

#### Pollinations (Gratuito)
```python
def generate_image_pollinations(prompt, style_settings):
    import requests
    from urllib.parse import quote
    
    # Pollinations usa URL simples
    encoded_prompt = quote(prompt)
    width = style_settings.get('width', 1024)
    height = style_settings.get('height', 1024)
    
    url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?width={width}&height={height}"
    
    response = requests.get(url)
    
    if response.status_code == 200:
        return response.content
    else:
        raise Exception(f"Erro Pollinations: {response.status_code}")
```

### An√°lise de Roteiro para Cenas

```python
def analyze_script_for_scenes(script_text):
    # Identifica cenas baseadas em estrutura do roteiro
    scenes = []
    
    # Divide por par√°grafos ou se√ß√µes
    paragraphs = script_text.split('\n\n')
    
    for i, paragraph in enumerate(paragraphs):
        if len(paragraph.strip()) < 50:  # Muito curto, pula
            continue
            
        # Extrai conceitos visuais
        visual_concepts = extract_visual_concepts(paragraph)
        
        # Identifica momento narrativo
        narrative_moment = identify_narrative_moment(paragraph, i, len(paragraphs))
        
        scene = {
            'index': i,
            'text': paragraph,
            'visual_concepts': visual_concepts,
            'narrative_moment': narrative_moment,
            'duration_estimate': estimate_duration(paragraph)
        }
        
        scenes.append(scene)
    
    return scenes

def extract_visual_concepts(text):
    # Identifica elementos visuais mencionados
    visual_keywords = {
        'pessoas': ['pessoa', 'homem', 'mulher', 'crian√ßa', 'jovem', 'adulto'],
        'objetos': ['computador', 'celular', 'livro', 'carro', 'casa'],
        'lugares': ['escrit√≥rio', 'casa', 'rua', 'parque', 'praia'],
        'a√ß√µes': ['trabalhando', 'estudando', 'correndo', 'sorrindo'],
        'emo√ß√µes': ['feliz', 'triste', 'surpreso', 'concentrado']
    }
    
    concepts = []
    text_lower = text.lower()
    
    for category, keywords in visual_keywords.items():
        found_keywords = [kw for kw in keywords if kw in text_lower]
        if found_keywords:
            concepts.append({
                'category': category,
                'keywords': found_keywords
            })
    
    return concepts

def identify_narrative_moment(text, index, total):
    # Identifica o tipo de momento narrativo
    if index == 0:
        return 'opening'  # Abertura
    elif index == total - 1:
        return 'closing'  # Fechamento
    elif index < total * 0.3:
        return 'introduction'  # Introdu√ß√£o
    elif index > total * 0.7:
        return 'conclusion'  # Conclus√£o
    else:
        return 'development'  # Desenvolvimento
```

### Gera√ß√£o de Prompts Visuais

```python
def generate_scene_prompts_with_ai(scenes, script_context):
    prompts = []
    
    for scene in scenes:
        # Prompt para IA gerar descri√ß√£o visual
        ai_prompt = f"""
        ## Contexto do V√≠deo
        T√≥pico: {script_context.get('topic', '')}
        Estilo: {script_context.get('style', 'educational')}
        P√∫blico: {script_context.get('audience', 'geral')}
        
        ## Texto da Cena
        "{scene['text'][:200]}..."
        
        ## Conceitos Visuais Identificados
        {json.dumps(scene['visual_concepts'], indent=2)}
        
        ## Momento Narrativo
        {scene['narrative_moment']}
        
        ## Tarefa
        Crie uma descri√ß√£o visual detalhada para esta cena que:
        
        1. **Represente visualmente** o conte√∫do falado
        2. **Seja apropriada** para o p√∫blico-alvo
        3. **Mantenha consist√™ncia** com o estilo do v√≠deo
        4. **Seja espec√≠fica** o suficiente para gerar uma imagem
        5. **Evite elementos** que possam ser controversos
        
        ## Formato
        Retorne apenas a descri√ß√£o visual, m√°ximo 100 palavras.
        Use linguagem descritiva e espec√≠fica.
        
        Exemplo: "Uma pessoa jovem sorrindo enquanto usa um laptop moderno em um escrit√≥rio bem iluminado, com plantas ao fundo e uma x√≠cara de caf√© na mesa, ambiente profissional e acolhedor"
        """
        
        # Chama IA para gerar prompt
        visual_prompt = call_ai_for_visual_prompt(ai_prompt)
        
        # Adiciona modificadores de estilo
        styled_prompt = apply_style_modifiers(visual_prompt, script_context)
        
        prompts.append({
            'scene_index': scene['index'],
            'original_text': scene['text'],
            'visual_prompt': styled_prompt,
            'narrative_moment': scene['narrative_moment']
        })
    
    return prompts

def apply_style_modifiers(base_prompt, context):
    style = context.get('image_style', 'realistic')
    
    style_modifiers = {
        'realistic': ', fotografia profissional, alta qualidade, ilumina√ß√£o natural',
        'cartoon': ', estilo cartoon, cores vibrantes, ilustra√ß√£o digital',
        'artistic': ', arte digital, estilo art√≠stico, composi√ß√£o criativa',
        'minimalist': ', estilo minimalista, cores suaves, composi√ß√£o limpa',
        'corporate': ', ambiente corporativo, profissional, moderno'
    }
    
    modifier = style_modifiers.get(style, '')
    
    # Adiciona qualidade e formato
    quality_modifier = ', 4K, alta resolu√ß√£o'
    format_modifier = context.get('format', 'landscape')
    
    if format_modifier == 'landscape':
        format_mod = ', formato paisagem 16:9'
    elif format_modifier == 'portrait':
        format_mod = ', formato retrato 9:16'
    else:
        format_mod = ', formato quadrado 1:1'
    
    return base_prompt + modifier + quality_modifier + format_mod
```

### Distribui√ß√£o Temporal de Cenas

```python
def distribute_scenes_evenly(scenes, total_duration):
    # Distribui cenas ao longo da dura√ß√£o total do v√≠deo
    if not scenes:
        return []
    
    scene_duration = total_duration / len(scenes)
    distributed_scenes = []
    
    for i, scene in enumerate(scenes):
        start_time = i * scene_duration
        end_time = (i + 1) * scene_duration
        
        distributed_scenes.append({
            **scene,
            'start_time': start_time,
            'end_time': end_time,
            'duration': scene_duration
        })
    
    return distributed_scenes
```

---

## 7. üé¨ Processo de Cria√ß√£o de V√≠deo

### Localiza√ß√£o
- **Arquivo**: `backend/services/video_creation_service.py`
- **Classe**: `VideoCreationService`
- **Endpoint**: `POST /api/videos/create`

### Depend√™ncias e Configura√ß√£o

```python
# Imports principais
from moviepy.editor import (
    VideoFileClip, ImageClip, AudioFileClip, CompositeVideoClip,
    TextClip, concatenate_videoclips, ColorClip
)
from moviepy.video.fx.resize import resize
from moviepy.video.fx.fadein import fadein
from moviepy.video.fx.fadeout import fadeout
from moviepy.audio.fx.volumex import volumex

# Configura√ß√£o PIL para compatibilidade
from PIL import Image
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS
```

### Normaliza√ß√£o de Resolu√ß√£o

```python
def _normalize_resolution(self, resolution):
    # Converte qualidades para resolu√ß√µes espec√≠ficas
    quality_to_resolution = {
        '720p': '1280x720',
        '1080p': '1920x1080',
        '4k': '3840x2160',
        'hd': '1280x720',
        'full_hd': '1920x1080',
        'ultra_hd': '3840x2160'
    }
    
    # Se j√° √© uma resolu√ß√£o v√°lida (formato WxH)
    if 'x' in resolution and self._is_valid_resolution(resolution):
        return resolution
    
    # Converte qualidade para resolu√ß√£o
    normalized = quality_to_resolution.get(resolution.lower(), '1920x1080')
    self._log('info', f'Resolu√ß√£o normalizada: {resolution} -> {normalized}')
    return normalized

def _is_valid_resolution(self, resolution):
    try:
        width, height = resolution.split('x')
        return width.isdigit() and height.isdigit()
    except:
        return False
```

### Sincroniza√ß√£o √Åudio-V√≠deo

```python
def _synchronize_audio_video(self, audio_path, images, tts_segments):
    # Carrega √°udio principal
    audio_clip = AudioFileClip(audio_path)
    total_duration = audio_clip.duration
    
    # Se temos segmentos TTS, usa timing preciso
    if tts_segments:
        return self._sync_with_tts_segments(images, tts_segments, total_duration)
    else:
        return self._sync_evenly(images, total_duration)

def _sync_with_tts_segments(self, images, tts_segments, total_duration):
    video_clips = []
    
    # Calcula quantas imagens por segmento
    images_per_segment = len(images) / len(tts_segments)
    
    for i, segment in enumerate(tts_segments):
        # Determina quais imagens usar neste segmento
        start_img_idx = int(i * images_per_segment)
        end_img_idx = int((i + 1) * images_per_segment)
        segment_images = images[start_img_idx:end_img_idx]
        
        if not segment_images:
            continue
            
        # Dura√ß√£o do segmento
        segment_duration = segment['end_time'] - segment['start_time']
        
        # Cria clipes para as imagens do segmento
        segment_clips = self._create_image_clips(
            segment_images, 
            segment_duration, 
            segment['start_time']
        )
        
        video_clips.extend(segment_clips)
    
    return video_clips

def _sync_evenly(self, images, total_duration):
    # Distribui imagens uniformemente
    if not images:
        return []
    
    duration_per_image = total_duration / len(images)
    video_clips = []
    
    for i, image_data in enumerate(images):
        start_time = i * duration_per_image
        clip = self._create_single_image_clip(
            image_data, 
            duration_per_image, 
            start_time
        )
        video_clips.append(clip)
    
    return video_clips
```

### Cria√ß√£o de Clipes de Imagem

```python
def _create_image_clips(self, images, total_duration, start_time):
    clips = []
    duration_per_image = total_duration / len(images) if images else 0
    
    for i, image_data in enumerate(images):
        image_start = start_time + (i * duration_per_image)
        clip = self._create_single_image_clip(
            image_data, 
            duration_per_image, 
            image_start
        )
        clips.append(clip)
    
    return clips

def _create_single_image_clip(self, image_data, duration, start_time):
    image_path = image_data.get('file_path') or image_data.get('path')
    
    if not image_path or not os.path.exists(image_path):
        # Cria imagem placeholder
        return self._create_placeholder_clip(duration, start_time)
    
    try:
        # Carrega imagem
        clip = ImageClip(image_path, duration=duration)
        
        # Redimensiona para resolu√ß√£o alvo
        target_width, target_height = self._get_target_dimensions()
        clip = clip.resize((target_width, target_height))
        
        # Define tempo de in√≠cio
        clip = clip.set_start(start_time)
        
        # Adiciona efeitos de transi√ß√£o
        if self.transitions_enabled:
            clip = self._add_transition_effects(clip)
        
        return clip
        
    except Exception as e:
        self._log('warning', f'Erro ao processar imagem {image_path}: {str(e)}')
        return self._create_placeholder_clip(duration, start_time)

def _create_placeholder_clip(self, duration, start_time):
    # Cria clip colorido como placeholder
    color = (50, 50, 50)  # Cinza escuro
    target_width, target_height = self._get_target_dimensions()
    
    clip = ColorClip(
        size=(target_width, target_height), 
        color=color, 
        duration=duration
    ).set_start(start_time)
    
    return clip
```

### Efeitos de Transi√ß√£o

```python
def _add_transition_effects(self, clip):
    # Adiciona fade in/out suave
    fade_duration = min(0.5, clip.duration / 4)  # M√°ximo 0.5s ou 1/4 da dura√ß√£o
    
    if clip.duration > fade_duration * 2:
        clip = fadein(clip, fade_duration)
        clip = fadeout(clip, fade_duration)
    
    return clip

def _add_advanced_transitions(self, clips):
    # Transi√ß√µes mais avan√ßadas entre clipes
    if len(clips) < 2:
        return clips
    
    transition_clips = []
    transition_duration = 0.3
    
    for i in range(len(clips) - 1):
        current_clip = clips[i]
        next_clip = clips[i + 1]
        
        # Ajusta timing para sobreposi√ß√£o
        overlap_start = current_clip.end - transition_duration
        next_clip = next_clip.set_start(overlap_start)
        
        # Adiciona fade out ao clip atual
        current_clip = fadeout(current_clip, transition_duration)
        
        # Adiciona fade in ao pr√≥ximo clip
        next_clip = fadein(next_clip, transition_duration)
        
        transition_clips.extend([current_clip, next_clip])
    
    return transition_clips
```

### Gera√ß√£o de Legendas

```python
def _add_subtitles(self, video_clip, script_text, tts_segments):
    if not self.subtitles_enabled:
        return video_clip
    
    subtitle_clips = []
    
    if tts_segments:
        # Usa timing preciso dos segmentos TTS
        for segment in tts_segments:
            subtitle_clip = self._create_subtitle_clip(
                segment['text'],
                segment['start_time'],
                segment['duration']
            )
            subtitle_clips.append(subtitle_clip)
    else:
        # Distribui texto uniformemente
        sentences = self._split_text_for_subtitles(script_text)
        duration_per_sentence = video_clip.duration / len(sentences)
        
        for i, sentence in enumerate(sentences):
            start_time = i * duration_per_sentence
            subtitle_clip = self._create_subtitle_clip(
                sentence,
                start_time,
                duration_per_sentence
            )
            subtitle_clips.append(subtitle_clip)
    
    # Combina legendas com v√≠deo
    if subtitle_clips:
        return CompositeVideoClip([video_clip] + subtitle_clips)
    
    return video_clip

def _create_subtitle_clip(self, text, start_time, duration):
    # Configura√ß√µes de estilo das legendas
    font_size = self._calculate_font_size()
    font_color = 'white'
    stroke_color = 'black'
    stroke_width = 2
    
    # Cria clip de texto
    txt_clip = TextClip(
        text,
        fontsize=font_size,
        color=font_color,
        stroke_color=stroke_color,
        stroke_width=stroke_width,
        font='Arial-Bold'
    ).set_position(('center', 'bottom')).set_duration(duration).set_start(start_time)
    
    return txt_clip

def _calculate_font_size(self):
    # Calcula tamanho da fonte baseado na resolu√ß√£o
    width, height = self._get_target_dimensions()
    
    if height >= 2160:  # 4K
        return 72
    elif height >= 1080:  # Full HD
        return 48
    elif height >= 720:  # HD
        return 36
    else:  # SD
        return 24
```

### Renderiza√ß√£o Final

```python
def _render_final_video(self, video_clips, audio_clip, output_path):
    try:
        # Combina todos os clipes de v√≠deo
        if len(video_clips) > 1:
            final_video = CompositeVideoClip(video_clips)
        else:
            final_video = video_clips[0]
        
        # Adiciona √°udio
        final_video = final_video.with_audio(audio_clip)
        
        # Configura√ß√µes de renderiza√ß√£o
        render_settings = self._get_render_settings()
        
        # Renderiza v√≠deo
        final_video.write_videofile(
            output_path,
            **render_settings,
            verbose=False,
            logger=None
        )
        
        # Limpa recursos
        final_video.close()
        audio_clip.close()
        for clip in video_clips:
            clip.close()
        
        return output_path
        
    except Exception as e:
        self._log('error', f'Erro na renderiza√ß√£o: {str(e)}')
        raise

def _get_render_settings(self):
    return {
        'codec': 'libx264',
        'audio_codec': 'aac',
        'temp_audiofile': 'temp-audio.m4a',
        'remove_temp': True,
        'fps': 30,
        'bitrate': '5000k'
    }
```

---

## 8. üéØ Processo de Orquestra√ß√£o

### Localiza√ß√£o
- **Arquivo**: `backend/services/pipeline_service.py`
- **Classe**: `PipelineService`
- **Arquivo**: `backend/pipeline_complete.py`

### Estados da Pipeline

```python
class PipelineState(Enum):
    IDLE = "idle"
    EXTRACTING = "extracting"
    GENERATING_TITLES = "generating_titles"
    GENERATING_PREMISE = "generating_premise"
    GENERATING_SCRIPT = "generating_script"
    PROCESSING_SCRIPT = "processing_script"
    GENERATING_TTS = "generating_tts"
    GENERATING_IMAGES = "generating_images"
    CREATING_VIDEO = "creating_video"
    COMPLETED = "completed"
    ERROR = "error"
```

### Fluxo Principal

```python
def execute_complete_pipeline(self, config):
    pipeline_id = str(uuid.uuid4())
    
    try:
        # Inicializa pipeline
        self._update_pipeline_status(pipeline_id, PipelineState.EXTRACTING)
        
        # Etapa 1: Extra√ß√£o
        extraction_result = self._execute_extraction(config)
        
        # Etapa 2: Gera√ß√£o de T√≠tulos
        self._update_pipeline_status(pipeline_id, PipelineState.GENERATING_TITLES)
        titles_result = self._execute_title_generation(extraction_result, config)
        
        # Etapa 3: Gera√ß√£o de Premissa
        self._update_pipeline_status(pipeline_id, PipelineState.GENERATING_PREMISE)
        premise_result = self._execute_premise_generation(titles_result, config)
        
        # Etapa 4: Gera√ß√£o de Roteiro
        self._update_pipeline_status(pipeline_id, PipelineState.GENERATING_SCRIPT)
        script_result = self._execute_script_generation(premise_result, config)
        
        # Etapa 5: Processamento de Roteiro
        self._update_pipeline_status(pipeline_id, PipelineState.PROCESSING_SCRIPT)
        processed_script = self._execute_script_processing(script_result, config)
        
        # Etapa 6: Gera√ß√£o de TTS
        self._update_pipeline_status(pipeline_id, PipelineState.GENERATING_TTS)
        tts_result = self._execute_tts_generation(processed_script, config)
        
        # Etapa 7: Gera√ß√£o de Imagens
        self._update_pipeline_status(pipeline_id, PipelineState.GENERATING_IMAGES)
        images_result = self._execute_image_generation(processed_script, config)
        
        # Etapa 8: Cria√ß√£o de V√≠deo
        self._update_pipeline_status(pipeline_id, PipelineState.CREATING_VIDEO)
        video_result = self._execute_video_creation(
            tts_result, images_result, processed_script, config
        )
        
        # Finaliza√ß√£o
        self._update_pipeline_status(pipeline_id, PipelineState.COMPLETED)
        
        return {
            'pipeline_id': pipeline_id,
            'status': 'completed',
            'results': {
                'extraction': extraction_result,
                'titles': titles_result,
                'premise': premise_result,
                'script': script_result,
                'tts': tts_result,
                'images': images_result,
                'video': video_result
            }
        }
        
    except Exception as e:
        self._update_pipeline_status(pipeline_id, PipelineState.ERROR)
        self._log_error(pipeline_id, str(e))
        raise
```

### Monitoramento e Logs

```python
def _update_pipeline_status(self, pipeline_id, state, progress=None, message=None):
    status_update = {
        'pipeline_id': pipeline_id,
        'state': state.value,
        'timestamp': datetime.now().isoformat(),
        'progress': progress,
        'message': message
    }
    
    # Salva no banco/cache
    self._save_pipeline_status(pipeline_id, status_update)
    
    # Emite evento para frontend (WebSocket)
    self._emit_status_update(status_update)

def _log_error(self, pipeline_id, error_message):
    error_log = {
        'pipeline_id': pipeline_id,
        'error': error_message,
        'timestamp': datetime.now().isoformat(),
        'stack_trace': traceback.format_exc()
    }
    
    # Salva log de erro
    self._save_error_log(error_log)
    
    # Notifica sistema de monitoramento
    self._notify_error(error_log)
```

---

## üîß Configura√ß√µes e Depend√™ncias

### Vari√°veis de Ambiente

```bash
# APIs de IA
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
OPENROUTER_API_KEY=...
TOGETHER_API_KEY=...
ELEVENLABS_API_KEY=...

# Configura√ß√µes de TTS
TTS_PROVIDER=gemini
TTS_VOICE_ID=pt-BR-Standard-A
TTS_SPEED=1.0
TTS_PITCH=0.0

# Configura√ß√µes de Imagem
IMAGE_PROVIDER=together
IMAGE_MODEL=black-forest-labs/FLUX.1-schnell
IMAGE_STYLE=realistic
IMAGE_QUALITY=high

# Configura√ß√µes de V√≠deo
VIDEO_RESOLUTION=1920x1080
VIDEO_FPS=30
VIDEO_BITRATE=5000k
SUBTITLES_ENABLED=true
TRANSITIONS_ENABLED=true

# Configura√ß√µes de Pipeline
MAX_CONCURRENT_PIPELINES=3
PIPELINE_TIMEOUT=3600
AUTO_CLEANUP=true
```

### Depend√™ncias Python

```txt
# Core
Flask==2.3.3
Flask-CORS==4.0.0
requests==2.31.0

# IA e APIs
openai==1.3.0
google-generativeai==0.3.0

# Processamento de M√≠dia
moviepy==1.0.3
Pillow==10.0.0
pydub==0.25.1

# Extra√ß√£o de Dados
yt-dlp==2023.9.24
beautifulsoup4==4.12.2

# Utilit√°rios
python-dotenv==1.0.0
psutil==5.9.5
colorama==0.4.6
```

### Estrutura de Arquivos

```
backend/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_service.py      # Orquestra√ß√£o principal
‚îÇ   ‚îú‚îÄ‚îÄ storyteller_service.py   # Gera√ß√£o de roteiros
‚îÇ   ‚îú‚îÄ‚îÄ tts_service.py           # Text-to-Speech
‚îÇ   ‚îú‚îÄ‚îÄ video_creation_service.py # Cria√ß√£o de v√≠deos
‚îÇ   ‚îî‚îÄ‚îÄ title_generator.py       # Gera√ß√£o de t√≠tulos
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ automations.py           # Rotas de automa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ images.py                # Rotas de imagens
‚îÇ   ‚îú‚îÄ‚îÄ videos.py                # Rotas de v√≠deos
‚îÇ   ‚îî‚îÄ‚îÄ scripts.py               # Rotas de roteiros
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ ai_clients.py            # Clientes de IA
‚îÇ   ‚îú‚îÄ‚îÄ file_manager.py          # Gerenciamento de arquivos
‚îÇ   ‚îî‚îÄ‚îÄ validators.py            # Valida√ß√µes
‚îî‚îÄ‚îÄ pipeline_complete.py         # Pipeline completa
```

---

## üö® Diagn√≥stico e Troubleshooting

### Logs de Sistema

```python
# Configura√ß√£o de logging
import logging
from datetime import datetime

class PipelineLogger:
    def __init__(self, pipeline_id):
        self.pipeline_id = pipeline_id
        self.logger = logging.getLogger(f'pipeline_{pipeline_id}')
        
        # Configurar handler para arquivo
        handler = logging.FileHandler(f'logs/pipeline_{pipeline_id}.log')
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_step(self, step, status, details=None):
        message = f'Step: {step} | Status: {status}'
        if details:
            message += f' | Details: {details}'
        
        if status == 'error':
            self.logger.error(message)
        elif status == 'warning':
            self.logger.warning(message)
        else:
            self.logger.info(message)
```

### Verifica√ß√µes de Sa√∫de

```python
def health_check():
    checks = {
        'apis': check_api_connectivity(),
        'storage': check_storage_space(),
        'dependencies': check_dependencies(),
        'services': check_services_status()
    }
    
    overall_health = all(checks.values())
    
    return {
        'healthy': overall_health,
        'checks': checks,
        'timestamp': datetime.now().isoformat()
    }

def check_api_connectivity():
    apis_to_check = [
        ('OpenAI', 'https://api.openai.com/v1/models'),
        ('Gemini', 'https://generativelanguage.googleapis.com'),
        ('ElevenLabs', 'https://api.elevenlabs.io/v1/voices')
    ]
    
    results = {}
    for name, url in apis_to_check:
        try:
            response = requests.get(url, timeout=5)
            results[name] = response.status_code < 400
        except:
            results[name] = False
    
    return all(results.values())
```

### C√≥digos de Erro Comuns

| C√≥digo | Descri√ß√£o | Solu√ß√£o |
|--------|-----------|----------|
| E001 | API Key inv√°lida | Verificar vari√°veis de ambiente |
| E002 | Quota de API excedida | Aguardar reset ou trocar provider |
| E003 | Arquivo n√£o encontrado | Verificar paths e permiss√µes |
| E004 | Erro de renderiza√ß√£o | Verificar FFmpeg e depend√™ncias |
| E005 | Timeout na pipeline | Aumentar timeout ou otimizar processo |
| E006 | Mem√≥ria insuficiente | Reduzir qualidade ou liberar recursos |
| E007 | Formato n√£o suportado | Converter arquivo para formato v√°lido |

---

## üìä M√©tricas e Performance

### Tempos M√©dios por Etapa

- **Extra√ß√£o**: 10-30 segundos
- **Gera√ß√£o de T√≠tulos**: 15-45 segundos
- **Gera√ß√£o de Premissa**: 10-30 segundos
- **Gera√ß√£o de Roteiro**: 60-180 segundos
- **Processamento TTS**: 30-120 segundos
- **Gera√ß√£o de Imagens**: 60-300 segundos
- **Cria√ß√£o de V√≠deo**: 120-600 segundos

**Total**: 5-20 minutos (dependendo da complexidade)

### Otimiza√ß√µes Implementadas

1. **Cache de Resultados**: Evita reprocessamento
2. **Processamento Paralelo**: TTS e imagens em paralelo
3. **Compress√£o de Imagens**: Reduz uso de storage
4. **Cleanup Autom√°tico**: Remove arquivos tempor√°rios
5. **Rate Limiting**: Evita bloqueios de API
6. **Retry Logic**: Recupera√ß√£o autom√°tica de falhas

---

Este documento fornece uma vis√£o completa e detalhada de cada processo da pipeline, permitindo diagn√≥stico preciso e manuten√ß√£o eficiente do sistema.